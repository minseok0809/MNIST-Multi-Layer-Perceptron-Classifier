{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 애니메이션 (구름빵) 캐릭터 얼굴 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[애니메이션 속 캐릭터 얼굴 랜드마크 데이터](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=5700)\n",
        "<br>[애니메이션 (구름빵) 캐릭터 얼굴 생성류](https://aifactory.space/task/2646/overview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G29mMs0Tf11v",
        "outputId": "0650097d-8b6c-46a0-8f0f-255b997a7998"
      },
      "outputs": [],
      "source": [
        "!pip install lmdb\n",
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waDr53TjhtaQ",
        "outputId": "f4b2aea8-7ec3-410f-d065-76772d7ca5df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Make dataset of image sizes: 64\n",
            "1000\n",
            "Processing:  data\n",
            "1000it [00:32, 30.78it/s]\n"
          ]
        }
      ],
      "source": [
        "!python modules/prepare_data.py data --out processed_data --size 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, autograd, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "import torch.distributed as dist\n",
        "from torchvision import transforms, utils\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    import wandb\n",
        "\n",
        "except ImportError:\n",
        "    wandb = None\n",
        "\n",
        "from modules.dataset import MultiResolutionDataset\n",
        "from torch.distributed import (\n",
        "    get_rank,\n",
        "    synchronize,\n",
        "    reduce_loss_dict,\n",
        "    reduce_sum,\n",
        "    get_world_size,\n",
        ")\n",
        "from op import conv2d_gradfix\n",
        "from non_leaking import augment, AdaptiveAugment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M_mEZADbhz-D"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dataset'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_1612686/114860481.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiResolutionDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m from distributed import (\n\u001b[1;32m     24\u001b[0m     \u001b[0mget_rank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
          ]
        }
      ],
      "source": [
        "def data_sampler(dataset, shuffle, distributed):\n",
        "    if distributed:\n",
        "        return data.distributed.DistributedSampler(dataset, shuffle=shuffle)\n",
        "\n",
        "    if shuffle:\n",
        "        return data.RandomSampler(dataset)\n",
        "\n",
        "    else:\n",
        "        return data.SequentialSampler(dataset)\n",
        "\n",
        "\n",
        "def requires_grad(model, flag=True):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = flag\n",
        "\n",
        "\n",
        "def accumulate(model1, model2, decay=0.999):\n",
        "    par1 = dict(model1.named_parameters())\n",
        "    par2 = dict(model2.named_parameters())\n",
        "\n",
        "    for k in par1.keys():\n",
        "        par1[k].data.mul_(decay).add_(par2[k].data, alpha=1 - decay)\n",
        "\n",
        "\n",
        "def sample_data(loader):\n",
        "    while True:\n",
        "        for batch in loader:\n",
        "            yield batch\n",
        "\n",
        "\n",
        "def d_logistic_loss(real_pred, fake_pred):\n",
        "    real_loss = F.softplus(-real_pred)\n",
        "    fake_loss = F.softplus(fake_pred)\n",
        "\n",
        "    return real_loss.mean() + fake_loss.mean()\n",
        "\n",
        "\n",
        "def d_r1_loss(real_pred, real_img):\n",
        "    with conv2d_gradfix.no_weight_gradients():\n",
        "        grad_real, = autograd.grad(\n",
        "            outputs=real_pred.sum(), inputs=real_img, create_graph=True\n",
        "        )\n",
        "    grad_penalty = grad_real.pow(2).reshape(grad_real.shape[0], -1).sum(1).mean()\n",
        "\n",
        "    return grad_penalty\n",
        "\n",
        "\n",
        "def g_nonsaturating_loss(fake_pred):\n",
        "    loss = F.softplus(-fake_pred).mean()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def g_path_regularize(fake_img, latents, mean_path_length, decay=0.01):\n",
        "    noise = torch.randn_like(fake_img) / math.sqrt(\n",
        "        fake_img.shape[2] * fake_img.shape[3]\n",
        "    )\n",
        "    grad, = autograd.grad(\n",
        "        outputs=(fake_img * noise).sum(), inputs=latents, create_graph=True\n",
        "    )\n",
        "    path_lengths = torch.sqrt(grad.pow(2).sum(2).mean(1))\n",
        "\n",
        "    path_mean = mean_path_length + decay * (path_lengths.mean() - mean_path_length)\n",
        "\n",
        "    path_penalty = (path_lengths - path_mean).pow(2).mean()\n",
        "\n",
        "    return path_penalty, path_mean.detach(), path_lengths\n",
        "\n",
        "\n",
        "def make_noise(batch, latent_dim, n_noise, device):\n",
        "    if n_noise == 1:\n",
        "        return torch.randn(batch, latent_dim, device=device)\n",
        "\n",
        "    noises = torch.randn(n_noise, batch, latent_dim, device=device).unbind(0)\n",
        "\n",
        "    return noises\n",
        "\n",
        "\n",
        "def mixing_noise(batch, latent_dim, prob, device):\n",
        "    if prob > 0 and random.random() < prob:\n",
        "        return make_noise(batch, latent_dim, 2, device)\n",
        "\n",
        "    else:\n",
        "        return [make_noise(batch, latent_dim, 1, device)]\n",
        "\n",
        "\n",
        "def set_grad_none(model, targets):\n",
        "    for n, p in model.named_parameters():\n",
        "        if n in targets:\n",
        "            p.grad = None\n",
        "\n",
        "\n",
        "def train(args, loader, generator, discriminator, g_optim, d_optim, g_ema, device):\n",
        "    os.makedirs(\"sample\", exist_ok=True)\n",
        "    os.makedirs(\"checkpoint\", exist_ok=True)\n",
        "    loader = sample_data(loader)\n",
        "\n",
        "    pbar = range(args.iter)\n",
        "\n",
        "    if get_rank() == 0:\n",
        "        pbar = tqdm(pbar, initial=args.start_iter, dynamic_ncols=True, smoothing=0.01)\n",
        "\n",
        "    mean_path_length = 0\n",
        "\n",
        "    d_loss_val = 0\n",
        "    r1_loss = torch.tensor(0.0, device=device)\n",
        "    g_loss_val = 0\n",
        "    path_loss = torch.tensor(0.0, device=device)\n",
        "    path_lengths = torch.tensor(0.0, device=device)\n",
        "    mean_path_length_avg = 0\n",
        "    loss_dict = {}\n",
        "\n",
        "    if args.distributed:\n",
        "        g_module = generator.module\n",
        "        d_module = discriminator.module\n",
        "\n",
        "    else:\n",
        "        g_module = generator\n",
        "        d_module = discriminator\n",
        "\n",
        "    accum = 0.5 ** (32 / (10 * 1000))\n",
        "    ada_aug_p = args.augment_p if args.augment_p > 0 else 0.0\n",
        "    r_t_stat = 0\n",
        "\n",
        "    if args.augment and args.augment_p == 0:\n",
        "        ada_augment = AdaptiveAugment(args.ada_target, args.ada_length, 8, device)\n",
        "\n",
        "    sample_z = torch.randn(args.n_sample, args.latent, device=device)\n",
        "\n",
        "    for idx in pbar:\n",
        "        i = idx + args.start_iter\n",
        "\n",
        "        if i > args.iter:\n",
        "            print(\"Done!\")\n",
        "\n",
        "            break\n",
        "\n",
        "        real_img = next(loader)\n",
        "        real_img = real_img.to(device)\n",
        "\n",
        "        requires_grad(generator, False)\n",
        "        requires_grad(discriminator, True)\n",
        "\n",
        "        noise = mixing_noise(args.batch, args.latent, args.mixing, device)\n",
        "        fake_img, _ = generator(noise)\n",
        "\n",
        "        if args.augment:\n",
        "            real_img_aug, _ = augment(real_img, ada_aug_p)\n",
        "            fake_img, _ = augment(fake_img, ada_aug_p)\n",
        "\n",
        "        else:\n",
        "            real_img_aug = real_img\n",
        "\n",
        "        fake_pred = discriminator(fake_img)\n",
        "        real_pred = discriminator(real_img_aug)\n",
        "        d_loss = d_logistic_loss(real_pred, fake_pred)\n",
        "\n",
        "        loss_dict[\"d\"] = d_loss\n",
        "        loss_dict[\"real_score\"] = real_pred.mean()\n",
        "        loss_dict[\"fake_score\"] = fake_pred.mean()\n",
        "\n",
        "        discriminator.zero_grad()\n",
        "        d_loss.backward()\n",
        "        d_optim.step()\n",
        "\n",
        "        if args.augment and args.augment_p == 0:\n",
        "            ada_aug_p = ada_augment.tune(real_pred)\n",
        "            r_t_stat = ada_augment.r_t_stat\n",
        "\n",
        "        d_regularize = i % args.d_reg_every == 0\n",
        "\n",
        "        if d_regularize:\n",
        "            real_img.requires_grad = True\n",
        "\n",
        "            if args.augment:\n",
        "                real_img_aug, _ = augment(real_img, ada_aug_p)\n",
        "\n",
        "            else:\n",
        "                real_img_aug = real_img\n",
        "\n",
        "            real_pred = discriminator(real_img_aug)\n",
        "            r1_loss = d_r1_loss(real_pred, real_img)\n",
        "\n",
        "            discriminator.zero_grad()\n",
        "            (args.r1 / 2 * r1_loss * args.d_reg_every + 0 * real_pred[0]).backward()\n",
        "\n",
        "            d_optim.step()\n",
        "\n",
        "        loss_dict[\"r1\"] = r1_loss\n",
        "\n",
        "        requires_grad(generator, True)\n",
        "        requires_grad(discriminator, False)\n",
        "\n",
        "        noise = mixing_noise(args.batch, args.latent, args.mixing, device)\n",
        "        fake_img, _ = generator(noise)\n",
        "\n",
        "        if args.augment:\n",
        "            fake_img, _ = augment(fake_img, ada_aug_p)\n",
        "\n",
        "        fake_pred = discriminator(fake_img)\n",
        "        g_loss = g_nonsaturating_loss(fake_pred)\n",
        "\n",
        "        loss_dict[\"g\"] = g_loss\n",
        "\n",
        "        generator.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optim.step()\n",
        "\n",
        "        g_regularize = i % args.g_reg_every == 0\n",
        "\n",
        "        if g_regularize:\n",
        "            path_batch_size = max(1, args.batch // args.path_batch_shrink)\n",
        "            noise = mixing_noise(path_batch_size, args.latent, args.mixing, device)\n",
        "            fake_img, latents = generator(noise, return_latents=True)\n",
        "\n",
        "            path_loss, mean_path_length, path_lengths = g_path_regularize(\n",
        "                fake_img, latents, mean_path_length\n",
        "            )\n",
        "\n",
        "            generator.zero_grad()\n",
        "            weighted_path_loss = args.path_regularize * args.g_reg_every * path_loss\n",
        "\n",
        "            if args.path_batch_shrink:\n",
        "                weighted_path_loss += 0 * fake_img[0, 0, 0, 0]\n",
        "\n",
        "            weighted_path_loss.backward()\n",
        "\n",
        "            g_optim.step()\n",
        "\n",
        "            mean_path_length_avg = (\n",
        "                reduce_sum(mean_path_length).item() / get_world_size()\n",
        "            )\n",
        "\n",
        "        loss_dict[\"path\"] = path_loss\n",
        "        loss_dict[\"path_length\"] = path_lengths.mean()\n",
        "\n",
        "        accumulate(g_ema, g_module, accum)\n",
        "\n",
        "        loss_reduced = reduce_loss_dict(loss_dict)\n",
        "\n",
        "        d_loss_val = loss_reduced[\"d\"].mean().item()\n",
        "        g_loss_val = loss_reduced[\"g\"].mean().item()\n",
        "        r1_val = loss_reduced[\"r1\"].mean().item()\n",
        "        path_loss_val = loss_reduced[\"path\"].mean().item()\n",
        "        real_score_val = loss_reduced[\"real_score\"].mean().item()\n",
        "        fake_score_val = loss_reduced[\"fake_score\"].mean().item()\n",
        "        path_length_val = loss_reduced[\"path_length\"].mean().item()\n",
        "\n",
        "        if get_rank() == 0:\n",
        "            pbar.set_description(\n",
        "                (\n",
        "                    f\"d: {d_loss_val:.4f}; g: {g_loss_val:.4f}; r1: {r1_val:.4f}; \"\n",
        "                    f\"path: {path_loss_val:.4f}; mean path: {mean_path_length_avg:.4f}; \"\n",
        "                    f\"augment: {ada_aug_p:.4f}\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            if wandb and args.wandb:\n",
        "                wandb.log(\n",
        "                    {\n",
        "                        \"Generator\": g_loss_val,\n",
        "                        \"Discriminator\": d_loss_val,\n",
        "                        \"Augment\": ada_aug_p,\n",
        "                        \"Rt\": r_t_stat,\n",
        "                        \"R1\": r1_val,\n",
        "                        \"Path Length Regularization\": path_loss_val,\n",
        "                        \"Mean Path Length\": mean_path_length,\n",
        "                        \"Real Score\": real_score_val,\n",
        "                        \"Fake Score\": fake_score_val,\n",
        "                        \"Path Length\": path_length_val,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                with torch.no_grad():\n",
        "                    g_ema.eval()\n",
        "                    sample, _ = g_ema([sample_z])\n",
        "                    utils.save_image(\n",
        "                        sample,\n",
        "                        f\"sample/{str(i).zfill(6)}.png\",\n",
        "                        nrow=int(args.n_sample ** 0.5),\n",
        "                        normalize=True,\n",
        "                        value_range=(-1, 1),\n",
        "                    )\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"g\": g_module.state_dict(),\n",
        "                        \"d\": d_module.state_dict(),\n",
        "                        \"g_ema\": g_ema.state_dict(),\n",
        "                        \"g_optim\": g_optim.state_dict(),\n",
        "                        \"d_optim\": d_optim.state_dict(),\n",
        "                        \"args\": args,\n",
        "                        \"ada_aug_p\": ada_aug_p,\n",
        "                    },\n",
        "                    f\"checkpoint/{str(i).zfill(6)}.pt\",\n",
        "                )\n",
        "    torch.save(\n",
        "        {\n",
        "            \"g\": g_module.state_dict(),\n",
        "            \"d\": d_module.state_dict(),\n",
        "            \"g_ema\": g_ema.state_dict(),\n",
        "            \"g_optim\": g_optim.state_dict(),\n",
        "            \"d_optim\": d_optim.state_dict(),\n",
        "            \"args\": args,\n",
        "            \"ada_aug_p\": ada_aug_p,\n",
        "        },\n",
        "        f\"checkpoint/last.pt\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRtT_4dyi24M"
      },
      "outputs": [],
      "source": [
        "class Args(object):\n",
        "    def __init__(self):\n",
        "        self.path = \"./processed_data\"\n",
        "        self.arch = \"stylegan2\"\n",
        "        self.iter = 10\n",
        "        self.batch = 16\n",
        "        self.n_sample = 25\n",
        "        self.size = 64\n",
        "        self.r1 = 64\n",
        "        self.path_regularize = 2\n",
        "        self.path_batch_shrink = 2\n",
        "        self.d_reg_every = 16\n",
        "        self.g_reg_every = 4\n",
        "        self.mixing = 0.9\n",
        "        self.ckpt = None\n",
        "        self.lr = 0.002\n",
        "        self.channel_multiplier = 2\n",
        "        self.local_rank = 0\n",
        "        self.augment = False\n",
        "        self.augment_p = 0\n",
        "        self.ada_target = 0.6\n",
        "        self.ada_length = 500 * 1000\n",
        "        self.ada_every = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu-8B8KpiwY1",
        "outputId": "8b4ed9ff-0044-4cff-e3ea-21aaaf79617b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d: 0.4030; g: 5.2789; r1: 0.0075; path: 0.6233; mean path: 0.0312; augment: 0.0000: 100%|██████████| 10/10 [00:25<00:00,  2.58s/it]\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\"\n",
        "\n",
        "args = Args()\n",
        "\n",
        "n_gpu = int(os.environ[\"WORLD_SIZE\"]) if \"WORLD_SIZE\" in os.environ else 1\n",
        "args.distributed = n_gpu > 1\n",
        "\n",
        "if args.distributed:\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
        "    synchronize()\n",
        "\n",
        "args.latent = 512\n",
        "args.n_mlp = 8\n",
        "\n",
        "args.start_iter = 0\n",
        "\n",
        "if args.arch == 'stylegan2':\n",
        "    from model import Generator, Discriminator\n",
        "\n",
        "elif args.arch == 'swagan':\n",
        "    from swagan import Generator, Discriminator\n",
        "\n",
        "generator = Generator(\n",
        "    args.size, args.latent, args.n_mlp, channel_multiplier=args.channel_multiplier\n",
        ").to(device)\n",
        "discriminator = Discriminator(\n",
        "    args.size, channel_multiplier=args.channel_multiplier\n",
        ").to(device)\n",
        "g_ema = Generator(\n",
        "    args.size, args.latent, args.n_mlp, channel_multiplier=args.channel_multiplier\n",
        ").to(device)\n",
        "g_ema.eval()\n",
        "accumulate(g_ema, generator, 0)\n",
        "\n",
        "g_reg_ratio = args.g_reg_every / (args.g_reg_every + 1)\n",
        "d_reg_ratio = args.d_reg_every / (args.d_reg_every + 1)\n",
        "\n",
        "g_optim = optim.Adam(\n",
        "    generator.parameters(),\n",
        "    lr=args.lr * g_reg_ratio,\n",
        "    betas=(0 ** g_reg_ratio, 0.99 ** g_reg_ratio),\n",
        ")\n",
        "d_optim = optim.Adam(\n",
        "    discriminator.parameters(),\n",
        "    lr=args.lr * d_reg_ratio,\n",
        "    betas=(0 ** d_reg_ratio, 0.99 ** d_reg_ratio),\n",
        ")\n",
        "\n",
        "if args.ckpt is not None:\n",
        "    print(\"load model:\", args.ckpt)\n",
        "\n",
        "    ckpt = torch.load(args.ckpt, map_location=lambda storage, loc: storage)\n",
        "\n",
        "    try:\n",
        "        ckpt_name = os.path.basename(args.ckpt)\n",
        "        args.start_iter = int(os.path.splitext(ckpt_name)[0])\n",
        "\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    generator.load_state_dict(ckpt[\"g\"])\n",
        "    discriminator.load_state_dict(ckpt[\"d\"])\n",
        "    g_ema.load_state_dict(ckpt[\"g_ema\"])\n",
        "\n",
        "    g_optim.load_state_dict(ckpt[\"g_optim\"])\n",
        "    d_optim.load_state_dict(ckpt[\"d_optim\"])\n",
        "\n",
        "if args.distributed:\n",
        "    generator = nn.parallel.DistributedDataParallel(\n",
        "        generator,\n",
        "        device_ids=[args.local_rank],\n",
        "        output_device=args.local_rank,\n",
        "        broadcast_buffers=False,\n",
        "    )\n",
        "\n",
        "    discriminator = nn.parallel.DistributedDataParallel(\n",
        "        discriminator,\n",
        "        device_ids=[args.local_rank],\n",
        "        output_device=args.local_rank,\n",
        "        broadcast_buffers=False,\n",
        "    )\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset = MultiResolutionDataset(args.path, transform, args.size)\n",
        "loader = data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=args.batch,\n",
        "    sampler=data_sampler(dataset, shuffle=True, distributed=args.distributed),\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "if get_rank() == 0 and wandb is not None and args.wandb:\n",
        "    wandb.init(project=\"stylegan 2\")\n",
        "\n",
        "train(args, loader, generator, discriminator, g_optim, d_optim, g_ema, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a6Znc_dk6Am"
      },
      "outputs": [],
      "source": [
        "class ArgsInfer(object):\n",
        "    def __init__(self):\n",
        "        self.ckpt = \"checkpoint/last.pt\"\n",
        "        self.result_path = \"fake_images\"\n",
        "        self.n_samples = 5000\n",
        "        self.truncation = 1\n",
        "        self.truncation_mean = 4096\n",
        "        self.size = 64\n",
        "        self.latent = 512\n",
        "        self.n_mlp = 8\n",
        "        self.channel_multiplier = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BtlciMJkwX6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "from model import Generator\n",
        "from torchvision import utils\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate(model, result_path, n_samples, args):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        os.makedirs(result_path, exist_ok=True)\n",
        "        for i in tqdm(range(n_samples), desc='Generating...'):\n",
        "            z = torch.randn(1, 512).cuda()\n",
        "            fake_x, _ = model([z], truncation=args.truncation, truncation_latent=None)\n",
        "            utils.save_image(fake_x, os.path.join(result_path, '{}.png'.format(i)), normalize=True, value_range=(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrDnop8wlc5O",
        "outputId": "0298e658-565a-4c3b-f859-9e5a15f00214"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating...: 100%|██████████| 5000/5000 [01:11<00:00, 69.66it/s]\n"
          ]
        }
      ],
      "source": [
        "args = ArgsInfer()\n",
        "torch.manual_seed(0)\n",
        "\n",
        "ckpt = torch.load(args.ckpt)\n",
        "\n",
        "model = Generator(args.size, args.latent, args.n_mlp).cuda()\n",
        "\n",
        "model.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
        "\n",
        "generate(model, args.result_path, args.n_samples, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkwodHaFm--O",
        "outputId": "77f524e3-b268-4224-ff1f-8432082748e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100% 104M/104M [00:00<00:00, 146MB/s]\n",
            "100% 100/100 [00:23<00:00,  4.18it/s]\n"
          ]
        }
      ],
      "source": [
        "!python extract_features_from_path.py ./fake_images fake.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd1wDEKhrsgZ",
        "outputId": "22ebe080-95a0-4f1b-b511-a8d6cf2c59eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100% 5000/5000 [00:08<00:00, 603.01it/s]\n"
          ]
        }
      ],
      "source": [
        "!python generate_randomized_fake_images.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVcZ6cqTr0gC",
        "outputId": "dcb307d7-e5d8-4d48-d1cc-8e5a45957e09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100% 100/100 [00:22<00:00,  4.39it/s]\n"
          ]
        }
      ],
      "source": [
        "!python extract_features_from_path.py ./fake_images/random random.pkl"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
