{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 한국어 지식 데이터의 관계 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[한국어 지식기반 관계 데이터](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=71633)\n",
        "<br>[한국어 지식 데이터의 관계 분류](https://aifactory.space/task/2658/leaderboard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hugginface Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x_path = 'data/train_x.csv'\n",
        "train_y_path = 'data/train_y.csv'\n",
        "test_x_path = 'data/test_x.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertModel\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast, BertModel\n",
        "\n",
        "Tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
        "\n",
        "def Tokenize(sentence, max_length = 64):\n",
        "    return Tokenizer.encode(sentence,padding = \"max_length\",truncation = True, max_length = max_length,return_tensors=\"pt\")\n",
        "\n",
        "class RelationClassificationModel(nn.Module):\n",
        "    def __init__(self, label_size= 20):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
        "        self.fc_subject = nn.Linear(768, 128)\n",
        "        self.fc_object = nn.Linear(768, 128)\n",
        "        self.fc_sentence = nn.Linear(768, 128)\n",
        "        self.attention = nn.MultiheadAttention(128, num_heads=4)\n",
        "        #self.proj = nn.Linear(24876,16384)\n",
        "        self.fc = nn.Linear(24576, 1024)\n",
        "        self.cl = nn.Linear(1024,label_size)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "    def forward(self, sentence, subject, object):\n",
        "\n",
        "        x_subject = self.bert(subject).last_hidden_state\n",
        "        x_object = self.bert(object).last_hidden_state\n",
        "        x_sentence = self.bert(sentence).last_hidden_state\n",
        "        x_subject = self.fc_subject(x_subject)\n",
        "        x_object = self.fc_object(x_object)\n",
        "        x_sentence = self.fc_sentence(x_sentence)\n",
        "\n",
        "        # attention을 사용하여 sentence의 정보를 반영합니다.\n",
        "        x = self.attention(x_subject, x_object, x_sentence)\n",
        "        x = x[0]\n",
        "\n",
        "        x = torch.cat((x,x_subject, x_object), dim=1)\n",
        "        x = x.flatten(1)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.cl(x)\n",
        "        return x\n",
        "\n",
        "class RDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence, subject, object, label = self.data[index]\n",
        "        return Tokenize(sentence),Tokenize(subject),Tokenize(object), label\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence, subject, object = self.data[index]\n",
        "        return Tokenize(sentence),Tokenize(subject),Tokenize(object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 20.6061    Train Accuracy: 0.162\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 19.9278    Train Accuracy: 0.165\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 19.6774    Train Accuracy: 0.178\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 19.5544    Train Accuracy: 0.178\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 19.2886    Train Accuracy: 0.186\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 18.3958    Train Accuracy: 0.232\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 8/8 [00:06<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 16.8916    Train Accuracy: 0.274\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 16.0165    Train Accuracy: 0.292\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 14.7736    Train Accuracy: 0.379\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 8/8 [00:06<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 13.9756    Train Accuracy: 0.411\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 12.4239    Train Accuracy: 0.467\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|██████████| 8/8 [00:06<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 11.0798    Train Accuracy: 0.531\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 9.9793    Train Accuracy: 0.554\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 8.5776    Train Accuracy: 0.625\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|██████████| 8/8 [00:06<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 7.6897    Train Accuracy: 0.67\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 6.0542    Train Accuracy: 0.753\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 4.7154    Train Accuracy: 0.811\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.5723    Train Accuracy: 0.854\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.9913    Train Accuracy: 0.898\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.1044    Train Accuracy: 0.92\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.7125    Train Accuracy: 0.939\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.1001    Train Accuracy: 0.97\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.9604    Train Accuracy: 0.966\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.7956    Train Accuracy: 0.978\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|██████████| 8/8 [00:06<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.7407    Train Accuracy: 0.975\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<bound method tqdm.close of <tqdm.std.tqdm object at 0x7fd5f012f9a0>>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "x_data_path = train_x_path\n",
        "y_data_path = train_y_path\n",
        "data_tuple = []\n",
        "data_x = pd.read_csv(x_data_path)\n",
        "data_y = pd.read_csv(y_data_path)\n",
        "if(len(data_x) != len(data_y)):\n",
        "    print(\"data length error\")\n",
        "else:\n",
        "    for i in range(len(data_x)):\n",
        "        temp = (data_x[\"sentence\"][i], data_x[\"subj\"][i], data_x[\"obj\"][i], data_y[\"label\"][i])\n",
        "        data_tuple.append(temp)\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "Rdataset = RDataset(data_tuple)\n",
        "dataloader = DataLoader(Rdataset, batch_size=batch_size,shuffle= True)\n",
        "\n",
        "\n",
        "# 모델 생성\n",
        "model = RelationClassificationModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#criterion = LabelSmoothingLoss(20, smoothing= 0.2)\n",
        "\n",
        "# 옵티마이저 생성\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.00003)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "epoch_num = 25\n",
        "\n",
        "for epoch in range(1, epoch_num + 1):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "    with tqdm(dataloader) as pbar: \n",
        "        pbar.set_description(\"Epoch \" + str(epoch))\n",
        "        for data in pbar:\n",
        "            model.train()\n",
        "            model = model.to(device)\n",
        "            sentence, subj, obj, label = data\n",
        "\n",
        "            sentence = sentence.reshape(sentence.shape[0],-1)\n",
        "            subj = subj.reshape(subj.shape[0],-1)\n",
        "            obj = obj.reshape(obj.shape[0],-1)\n",
        "            sentence = sentence.to(device)\n",
        "            subj = subj.to(device)\n",
        "            obj = obj.to(device)\n",
        "            label = label.to(device)\n",
        "            output = model(sentence, subj, obj)\n",
        "            loss = criterion(output, label)\n",
        "            total_loss += loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            output = torch.argmax(output,dim = 1)\n",
        "            output = output.tolist()\n",
        "            for p, y in zip(output, label):\n",
        "                if p == int(y):\n",
        "                    correct += 1\n",
        "            total += len(label)\n",
        "        \n",
        "        accuracy = round(correct / total, 4)\n",
        "        print(\"Train Loss:\", round(total_loss.item(), 4),  \"  \",  \"Train Accuracy:\", accuracy)\n",
        "        print()\n",
        "        torch.save(model,\"model_\" + str(epoch) + \".pt\")\n",
        "pbar.close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_tuple = []\n",
        "data_x = pd.read_csv(test_x_path)\n",
        "\n",
        "for i in range(len(data_x)):\n",
        "    temp = (data_x[\"sentence\"][i], data_x[\"subj\"][i], data_x[\"obj\"][i])\n",
        "    data_tuple.append(temp)\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "dataset = TestDataset(data_tuple)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size,shuffle= False)\n",
        "\n",
        "model_path = \"model_25.pt\"\n",
        "model = torch.load(model_path)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "results = []\n",
        "\n",
        "for data in iter(dataloader):\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    sentence,subj,obj = data\n",
        "\n",
        "    sentence = sentence.reshape(sentence.shape[0],-1)\n",
        "    subj = subj.reshape(subj.shape[0],-1)\n",
        "    obj = obj.reshape(obj.shape[0],-1)\n",
        "\n",
        "\n",
        "    sentence = sentence.to(device)\n",
        "    subj = subj.to(device)\n",
        "    obj = obj.to(device)\n",
        "\n",
        "    output = model(sentence,subj,obj)\n",
        "\n",
        "    output = torch.argmax(output,dim = 1)\n",
        "    output = output.tolist()\n",
        "    results.extend(output)\n",
        "\n",
        "\n",
        "result_csv = pd.DataFrame(results)\n",
        "result_csv.columns = {\"label\"}\n",
        "\n",
        "result_csv.to_csv(\"result.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Huggingface Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from safetensors.torch import load_model, save_model\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ntrain_sentences= []\\nfor i in tqdm(train_data['sentence']):\\n    train_sentences.append(spacing(i))\\ntrain_data['sentence'] = train_sentences\\n\\ntest_sentences = []\\nfor i in tqdm(test_data['sentence']):\\n    test_sentences.append(spacing(i))\\ntest_data['sentence'] = test_sentences\\n\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = pd.read_csv('data/train_x.csv', encoding='utf-8')\n",
        "train_y = pd.read_csv('data/train_y.csv', encoding='utf-8')\n",
        "train_data['label'] = train_y['label']\n",
        "test_data = pd.read_csv('data/test_x.csv', encoding='utf-8')\n",
        "\n",
        "train_data['instruction'] = \"문장에서 \" + train_data['subj'] + \" 그리고 \" + train_data['obj'] + \"는 관계가 있다\"\n",
        "test_data['instruction'] = \"문장에서 \" + test_data['subj'] + \" 그리고 \" + test_data['obj'] + \"는 관계가 있다\"\n",
        "train_data['sentence'] = train_data['sentence'].str.replace('[^A-Za-z0-9ㄱ-ㅎ가-힣]', ' ', regex=True)\n",
        "test_data['sentence'] = test_data['sentence'].str.replace('[^A-Za-z0-9ㄱ-ㅎ가-힣]', ' ', regex=True)\n",
        "\n",
        "\"\"\"\n",
        "train_sentences= []\n",
        "for i in tqdm(train_data['sentence']):\n",
        "    train_sentences.append(spacing(i))\n",
        "train_data['sentence'] = train_sentences\n",
        "\n",
        "test_sentences = []\n",
        "for i in tqdm(test_data['sentence']):\n",
        "    test_sentences.append(spacing(i))\n",
        "test_data['sentence'] = test_sentences\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max Sentence Length:  497\n",
            "Min Sentence Length:  13\n",
            "Mean Sentence Lengh:  84.307\n",
            "Max Instruction Length:  62\n",
            "Min Instruction Length:  21\n",
            "Mean Instruction Lengh:  29.139\n",
            "The Number of Label:  19\n"
          ]
        }
      ],
      "source": [
        "sentence_max_len = np.max(train_data['sentence'].str.len())\n",
        "sentence_min_len = np.min(train_data['sentence'].str.len())\n",
        "sentence_mean_len = np.mean(train_data['sentence'].str.len())\n",
        "instruction_max_len = np.max(train_data['instruction'].str.len())\n",
        "instruction_min_len = np.min(train_data['instruction'].str.len())\n",
        "instruction_mean_len = np.mean(train_data['instruction'].str.len())\n",
        "label_num = len(set(train_data['label']))\n",
        "\n",
        "print('Max Sentence Length: ', sentence_max_len)\n",
        "print('Min Sentence Length: ', sentence_min_len)\n",
        "print('Mean Sentence Lengh: ', sentence_mean_len)\n",
        "print('Max Instruction Length: ', instruction_max_len)\n",
        "print('Min Instruction Length: ', instruction_min_len)\n",
        "print('Mean Instruction Lengh: ', instruction_mean_len)\n",
        "print('The Number of Label: ', label_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'kykim/bert-kor-base' # 'kykim/bert-kor-base'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "config.num_labels = 20\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset, eval_dataset = train_test_split(train_data, test_size=0.1, shuffle=True, stratify=train_data['label'])\n",
        "\n",
        "tokenized_train = tokenizer(\n",
        "    list(train_dataset['sentence']),\n",
        "    list(train_dataset['instruction']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=512, \n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "tokenized_eval = tokenizer(\n",
        "    list(eval_dataset['sentence']),\n",
        "    list(eval_dataset['instruction']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=512,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "# print(tokenized_train['input_ids'][0])\n",
        "# print(tokenizer.decode(tokenized_train['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, pair_dataset, label):\n",
        "        self.pair_dataset = pair_dataset\n",
        "        self.label = label\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
        "        item['label'] = torch.tensor(self.label[idx])\n",
        "        \n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = BERTDataset(tokenized_train, train_dataset['label'].values)\n",
        "eval_dataset = BERTDataset(tokenized_eval, eval_dataset['label'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "  \n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  probs = pred.predictions\n",
        "\n",
        "  acc = accuracy_score(labels, preds) \n",
        "\n",
        "  return {\n",
        "      'accuracy': acc,\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_ars = TrainingArguments(\n",
        "    output_dir='output',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    save_total_limit=10,\n",
        "    save_strategy='epoch', \n",
        "    evaluation_strategy='epoch',\n",
        "    load_best_model_at_end = True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_ars,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics, \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 01:17, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.314619</td>\n",
              "      <td>0.260000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.146322</td>\n",
              "      <td>0.260000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.993048</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.858849</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.769520</td>\n",
              "      <td>0.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.756153</td>\n",
              "      <td>0.470000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.826705</td>\n",
              "      <td>0.430000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.848759</td>\n",
              "      <td>0.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.872167</td>\n",
              "      <td>0.490000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.896422</td>\n",
              "      <td>0.490000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = 'output' # 'kykim/bert-kor-base'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_safetensors=True)\n",
        "\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "config.num_labels = 20\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_test = tokenizer(\n",
        "    list(test_data['sentence']),\n",
        "    list(test_data['instruction']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=512, \n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataset = BERTDataset(tokenized_test, train_dataset['label'].values[0:len(test_data['sentence'])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_args = TrainingArguments(\n",
        "    output_dir = 'test_output',\n",
        "    do_train = False,\n",
        "    do_predict = True,\n",
        "    per_device_eval_batch_size = 32,   \n",
        "    dataloader_drop_last = False    \n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "              model = model, \n",
        "              args = test_args, \n",
        "              compute_metrics = compute_metrics)\n",
        "\n",
        "predictions, labels, metrics = trainer.predict(test_dataset)\n",
        "predictions = np.argmax(predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_csv = pd.DataFrame(labels)\n",
        "result_csv.columns = {\"label\"}\n",
        "\n",
        "result_csv.to_csv(\"result.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[월간 데이콘 한국어 문장 관계 분류 경진대회 - Hugging Face를 활용한 Modeling(public: 0.841)](https://github.com/ldj7672/Deep-Learning-Tutorials/blob/main/HuggingFace/HuggingFace_SwinT_image_classification.ipynb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
