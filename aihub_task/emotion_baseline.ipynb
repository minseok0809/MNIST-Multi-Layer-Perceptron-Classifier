{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 감정분류 모델 태스크"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[한국인 감정인식을 위한 복합 영상](https://aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=820)\n",
        "<br>[감정분류 모델 태스크](https://aifactory.space/task/2674/overvieww)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow==2.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-15 06:22:27.437113: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-15 06:22:27.742341: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 06:22:28.608941: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-11-15 06:22:28.609066: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-11-15 06:22:28.609076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Teansorflow: 2.10.0\n",
            "\n",
            "True\n",
            "\n",
            "Num GPUs Available:  2\n",
            "\n",
            "/device:GPU:0\n",
            "\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 7589890277320109501\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 23542562816\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 16291562385041011195\n",
            "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\"\n",
            "xla_global_id: 416903419\n",
            ", name: \"/device:GPU:1\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 23542562816\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 2008312571556366386\n",
            "physical_device_desc: \"device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:b3:00.0, compute capability: 8.6\"\n",
            "xla_global_id: 2144165316\n",
            "]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-15 06:22:30.236021: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-15 06:22:30.383720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 22451 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
            "2023-11-15 06:22:30.384571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:1 with 22451 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:b3:00.0, compute capability: 8.6\n",
            "2023-11-15 06:22:30.386907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 22451 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
            "2023-11-15 06:22:30.387055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:1 with 22451 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:b3:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "print(\"Teansorflow:\", tf.__version__)\n",
        "print()\n",
        "print(tf.test.is_built_with_cuda())\n",
        "print()\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print()\n",
        "print(tf.test.gpu_device_name())\n",
        "print()\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rYTIyedFUSOJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4171 images belonging to 7 classes.\n",
            "Epoch 1/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 2.1428 - accuracy: 0.2361\n",
            "Epoch 2/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 1.7341 - accuracy: 0.3511\n",
            "Epoch 3/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 1.5535 - accuracy: 0.4084\n",
            "Epoch 4/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 1.4758 - accuracy: 0.4477\n",
            "Epoch 5/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 1.4659 - accuracy: 0.4505\n",
            "Epoch 6/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 1.5412 - accuracy: 0.4243\n",
            "Epoch 7/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 1.5893 - accuracy: 0.3945\n",
            "Epoch 8/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 1.2863 - accuracy: 0.5237\n",
            "Epoch 9/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 1.2905 - accuracy: 0.5155\n",
            "Epoch 10/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 1.1548 - accuracy: 0.5805\n",
            "Epoch 11/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 0.9404 - accuracy: 0.6585\n",
            "Epoch 12/20\n",
            "260/260 [==============================] - 19s 73ms/step - loss: 0.7524 - accuracy: 0.7377\n",
            "Epoch 13/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 0.5627 - accuracy: 0.8161\n",
            "Epoch 14/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 0.4483 - accuracy: 0.8566\n",
            "Epoch 15/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 0.4319 - accuracy: 0.8585\n",
            "Epoch 16/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 0.4777 - accuracy: 0.8378\n",
            "Epoch 17/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 0.2230 - accuracy: 0.9307\n",
            "Epoch 18/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 0.1430 - accuracy: 0.9588\n",
            "Epoch 19/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 0.1239 - accuracy: 0.9651\n",
            "Epoch 20/20\n",
            "260/260 [==============================] - 19s 72ms/step - loss: 0.1387 - accuracy: 0.9576\n",
            "Model saved to: model.h5\n"
          ]
        }
      ],
      "source": [
        "x_train_path = 'dataset/Train'\n",
        "y_train_path = 'dataset/Train.csv'\n",
        "#model_save_path = sys.argv[3]\n",
        "\n",
        "# 데이터 경로 설정\n",
        "data_dir = x_train_path\n",
        "batch_size = 16\n",
        "image_size = (255, 255)\n",
        "num_classes = 7  # 7가지 감정 클래스\n",
        "\n",
        "# 데이터 증강 및 전처리\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255\n",
        "    # rotation_range=20,\n",
        "    # width_shift_range=0.2,\n",
        "    # height_shift_range=0.2,\n",
        "    # shear_range=0.2,\n",
        "    # horizontal_flip=True,\n",
        "    # fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "\"\"\"\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(255, 255, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# the model so far outputs 3D feature maps (height, width, features)\n",
        "\n",
        "# 모델 생성\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(255, 255, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')  # 출력 레이어: 클래스 수에 맞게 설정\n",
        "    ])\n",
        "\"\"\"\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "    model = keras.models.Sequential([\n",
        "        layers.InputLayer(input_shape=(255, 255, 3)),\n",
        "        keras.layers.Conv2D(64, 3, activation='relu', padding='SAME'), #cnn layer\n",
        "        keras.layers.BatchNormalization(), #batch norm layer\n",
        "        keras.layers.Conv2D(64, 3, activation='relu', padding='SAME'), #cnn layer\n",
        "        keras.layers.BatchNormalization(), #batch norm layer\n",
        "        keras.layers.Conv2D(64, 3, activation='relu', padding='SAME'), #cnn layer\n",
        "        keras.layers.BatchNormalization(), #batch norm layer\n",
        "\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2)), #pooling layer\n",
        "        # keras.layers.Dropout(0.5),\n",
        "\n",
        "        keras.layers.Flatten(),\n",
        "\n",
        "        keras.layers.Dense(128, activation=\"relu\"),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        # keras.layers.Dropout(0.2),\n",
        "\n",
        "        keras.layers.Dense(num_classes, activation=\"softmax\") # ouput layer\n",
        "        ])\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "epochs = 20\n",
        "\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=train_generator.samples // batch_size\n",
        "    \n",
        ")\n",
        "\n",
        "# 학습된 모델 저장\n",
        "model_save_path = \"model.h5\"  # Set the desired path for saving the model\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved to: {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yU5yjIJ8KxJW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17/17 [==============================] - 2s 116ms/step\n"
          ]
        }
      ],
      "source": [
        "model_path = model_save_path\n",
        "x_test_path = \"dataset/Test\"\n",
        "y_pred_save_path = \"y_pred.csv\"\n",
        "\n",
        "class Dataloader(Sequence):\n",
        "    def __init__(self, base_dataset_path, images, batch_size):\n",
        "        self.base_dataset_path = base_dataset_path\n",
        "        self.images = images\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = np.arange(len(self.images))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.images) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        indices = self.indices[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "        batch_x = [self.images[i] for i in indices]\n",
        "        batch_images = self.get_imagesets(batch_x)\n",
        "        batch_images = batch_images.astype('float32') / 255.0\n",
        "        return batch_images\n",
        "\n",
        "    def get_imagesets(self, path_list):\n",
        "        image_list = []\n",
        "        for image in path_list:\n",
        "            image_path = os.path.join(self.base_dataset_path, str(image))\n",
        "            image_list.append(cv2.imread(image_path))\n",
        "        return np.array(image_list)\n",
        "\n",
        "def main():\n",
        "    # 모델을 로드하기\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    inference_dataset = Dataloader(x_test_path, os.listdir(x_test_path), 64)\n",
        "    pred = model.predict(inference_dataset)\n",
        "    y_pred_labels = np.argmax(pred, axis=1)\n",
        "\n",
        "    df = pd.DataFrame(os.listdir(x_test_path), columns=['shuffle_데이터 경로'])\n",
        "    df['라벨'] = y_pred_labels\n",
        "\n",
        "    # 모델 추론 결과 저장\n",
        "    df.to_csv(y_pred_save_path, index=False, encoding='CP949')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
