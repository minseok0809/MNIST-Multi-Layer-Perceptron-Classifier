{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 감정분류 모델 태스크"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[한국인 감정인식을 위한 복합 영상](https://aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=820)\n",
        "<br>[감정분류 모델 태스크](https://aifactory.space/task/2674/overvieww)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rYTIyedFUSOJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4171 images belonging to 7 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-13 11:22:12.714585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18912 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
            "2023-11-13 11:22:12.715230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22395 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:b3:00.0, compute capability: 8.6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-13 11:22:13.899905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
            "2023-11-13 11:22:15.400052: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8179c29c90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-11-13 11:22:15.400075: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
            "2023-11-13 11:22:15.400080: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
            "2023-11-13 11:22:15.403857: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-11-13 11:22:15.486138: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "130/130 [==============================] - 11s 54ms/step - loss: 1.9608 - accuracy: 0.1500\n",
            "Epoch 2/15\n",
            "130/130 [==============================] - 7s 50ms/step - loss: 1.8711 - accuracy: 0.2438\n",
            "Epoch 3/15\n",
            "130/130 [==============================] - 7s 50ms/step - loss: 1.7394 - accuracy: 0.3090\n",
            "Epoch 4/15\n",
            "130/130 [==============================] - 7s 51ms/step - loss: 1.6562 - accuracy: 0.3409\n",
            "Epoch 5/15\n",
            "130/130 [==============================] - 7s 54ms/step - loss: 1.5762 - accuracy: 0.3897\n",
            "Epoch 6/15\n",
            "130/130 [==============================] - 7s 54ms/step - loss: 1.4915 - accuracy: 0.4088\n",
            "Epoch 7/15\n",
            "130/130 [==============================] - 7s 54ms/step - loss: 1.3765 - accuracy: 0.4576\n",
            "Epoch 8/15\n",
            "130/130 [==============================] - 7s 56ms/step - loss: 1.2986 - accuracy: 0.4934\n",
            "Epoch 9/15\n",
            "130/130 [==============================] - 6s 48ms/step - loss: 1.2015 - accuracy: 0.5250\n",
            "Epoch 10/15\n",
            "130/130 [==============================] - 7s 54ms/step - loss: 1.1019 - accuracy: 0.5666\n",
            "Epoch 11/15\n",
            "130/130 [==============================] - 7s 51ms/step - loss: 1.0194 - accuracy: 0.5934\n",
            "Epoch 12/15\n",
            "130/130 [==============================] - 7s 55ms/step - loss: 0.9268 - accuracy: 0.6265\n",
            "Epoch 13/15\n",
            "130/130 [==============================] - 7s 50ms/step - loss: 0.8760 - accuracy: 0.6531\n",
            "Epoch 14/15\n",
            "130/130 [==============================] - 7s 54ms/step - loss: 0.8081 - accuracy: 0.6765\n",
            "Epoch 15/15\n",
            "130/130 [==============================] - 6s 49ms/step - loss: 0.7507 - accuracy: 0.6944\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/anaconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: model.h5\n"
          ]
        }
      ],
      "source": [
        "x_train_path = 'dataset/Train'\n",
        "y_train_path = 'dataset/Train.csv'\n",
        "#model_save_path = sys.argv[3]\n",
        "\n",
        "# 데이터 경로 설정\n",
        "data_dir = x_train_path\n",
        "batch_size = 32\n",
        "image_size = (255, 255)\n",
        "num_classes = 7  # 7가지 감정 클래스\n",
        "\n",
        "# 데이터 증강 및 전처리\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255\n",
        "    # rotation_range=20,\n",
        "    # width_shift_range=0.2,\n",
        "    # height_shift_range=0.2,\n",
        "    # shear_range=0.2,\n",
        "    # horizontal_flip=True,\n",
        "    # fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(255, 255, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# the model so far outputs 3D feature maps (height, width, features)\n",
        "\n",
        "\"\"\"\n",
        "# 모델 생성\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(255, 255, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')  # 출력 레이어: 클래스 수에 맞게 설정\n",
        "])\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "model = keras.models.Sequential([\n",
        "    layers.InputLayer(input_shape=(255, 255, 3)),\n",
        "    keras.layers.Conv2D(64, 3, activation='relu', padding='SAME'), #cnn layer\n",
        "    keras.layers.BatchNormalization(), #batch norm layer\n",
        "    keras.layers.Conv2D(64, 3, activation='relu', padding='SAME'), #cnn layer\n",
        "    keras.layers.BatchNormalization(), #batch norm layer\n",
        "    keras.layers.Conv2D(64, 3, activation='relu', padding='SAME'), #cnn layer\n",
        "    keras.layers.BatchNormalization(), #batch norm layer\n",
        "\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)), #pooling layer\n",
        "    keras.layers.Dropout(0.25),\n",
        "\n",
        "    keras.layers.Conv2D(128, 3, activation='relu', padding='SAME'), #cnn layer\n",
        "    keras.layers.BatchNormalization(), #batch norm layer\n",
        "    keras.layers.Conv2D(128, 3, activation='relu', padding='SAME'), #cnn layer\n",
        "    keras.layers.BatchNormalization(), #batch norm layer\n",
        "    keras.layers.Conv2D(128, 3, activation='relu', padding='SAME'), #cnn layer\n",
        "    keras.layers.BatchNormalization(), #batch norm layer\n",
        "\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)), #pooling layer\n",
        "    keras.layers.Dropout(0.25),\n",
        "\n",
        "    keras.layers.Conv2D(256, 3, activation='relu', padding='SAME'), #cnn layer\n",
        "    keras.layers.BatchNormalization(), #batch norm layer\n",
        "    keras.layers.Conv2D(256, 3, activation='relu', padding='SAME'), #cnn layer\n",
        "    keras.layers.BatchNormalization(), #batch norm layer\n",
        "    keras.layers.Conv2D(256, 3, activation='relu', padding='SAME'), #cnn layer\n",
        "    keras.layers.BatchNormalization(), #batch norm layer\n",
        "    keras.layers.Conv2D(256, 3, activation='relu', padding='SAME'), #cnn layer\n",
        "    keras.layers.BatchNormalization(), #batch norm layer\n",
        "\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)), #pooling layer\n",
        "    keras.layers.Dropout(0.5),\n",
        "\n",
        "    keras.layers.Flatten(),\n",
        "\n",
        "    keras.layers.Dense(512, activation=\"relu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.2),\n",
        "\n",
        "    keras.layers.Dense(num_classes, activation=\"softmax\") # ouput layer\n",
        "    ])\n",
        "\"\"\"\n",
        "# 모델 컴파일\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "epochs = 20\n",
        "\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=train_generator.samples // batch_size\n",
        ")\n",
        "\n",
        "# 학습된 모델 저장\n",
        "model_save_path = \"model.h5\"  # Set the desired path for saving the model\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved to: {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yU5yjIJ8KxJW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17/17 [==============================] - 2s 114ms/step\n"
          ]
        }
      ],
      "source": [
        "model_path = model_save_path\n",
        "x_test_path = \"dataset/Test\"\n",
        "y_pred_save_path = \"y_pred.csv\"\n",
        "\n",
        "class Dataloader(Sequence):\n",
        "    def __init__(self, base_dataset_path, images, batch_size):\n",
        "        self.base_dataset_path = base_dataset_path\n",
        "        self.images = images\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = np.arange(len(self.images))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.images) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        indices = self.indices[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "        batch_x = [self.images[i] for i in indices]\n",
        "        batch_images = self.get_imagesets(batch_x)\n",
        "        batch_images = batch_images.astype('float32') / 255.0\n",
        "        return batch_images\n",
        "\n",
        "    def get_imagesets(self, path_list):\n",
        "        image_list = []\n",
        "        for image in path_list:\n",
        "            image_path = os.path.join(self.base_dataset_path, str(image))\n",
        "            image_list.append(cv2.imread(image_path))\n",
        "        return np.array(image_list)\n",
        "\n",
        "def main():\n",
        "    # 모델을 로드하기\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    inference_dataset = Dataloader(x_test_path, os.listdir(x_test_path), 64)\n",
        "    pred = model.predict(inference_dataset)\n",
        "    y_pred_labels = np.argmax(pred, axis=1)\n",
        "\n",
        "    df = pd.DataFrame(os.listdir(x_test_path), columns=['shuffle_데이터 경로'])\n",
        "    df['라벨'] = y_pred_labels\n",
        "\n",
        "    # 모델 추론 결과 저장\n",
        "    df.to_csv(y_pred_save_path, index=False, encoding='CP949')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
