{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a59X-OjRmPHS"
      },
      "outputs": [],
      "source": [
        "x_train_path = \"/content/x_train.npy\"\n",
        "y_train_path = \"/content/y_train.csv\"\n",
        "x_test_path = \"/content/x_test.npy\"\n",
        "model_save_path = \"/content/model.pth\"\n",
        "model_path = \"/content/model.pth\"\n",
        "y_tmp_save_path = \"/content/y_tmp.csv\"\n",
        "y_pred_save_path = \"/content/y_pred.csv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59TpMOC121hm",
        "outputId": "501b8b78-db35-4ce7-83e8-91e8202522d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: gdown [-h] [-V] [-O OUTPUT] [-q] [--fuzzy] [--id] [--proxy PROXY] [--speed SPEED]\n",
            "             [--no-cookies] [--no-check-certificate] [--continue] [--folder] [--remaining-ok]\n",
            "             url_or_id\n",
            "gdown: error: the following arguments are required: url_or_id\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = \"1Q4MQuo7_fcu2RIgIgzSH4GgFYDx5kvla\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\""
      ],
      "metadata": {
        "id": "cWB3f7a42uMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 다운로드 전의 파일 리스트를 가져옵니다.\n",
        "before_files = set(os.listdir())\n",
        "\n",
        "# gdown을 사용하여 파일을 다운로드합니다.\n",
        "!gdown {url}\n",
        "\n",
        "# 다운로드 후의 파일 리스트를 가져옵니다.\n",
        "after_files = set(os.listdir())\n",
        "\n",
        "# 다운로드된 파일명을 찾습니다.\n",
        "downloaded_files = after_files - before_files\n",
        "if downloaded_files:\n",
        "    filename = downloaded_files.pop()\n",
        "    print(f\"Downloaded file: {filename}\")\n",
        "    downloaded_filepath = filename\n",
        "else:\n",
        "    print(\"No file downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwKU_Fsf3BWW",
        "outputId": "ad6488c0-82c9-4a0e-85c2-bd8b48dd34fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Q4MQuo7_fcu2RIgIgzSH4GgFYDx5kvla\n",
            "To: /content/ginseng_competition_data.zip\n",
            "100% 391M/391M [00:08<00:00, 45.6MB/s]\n",
            "No file downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip {downloaded_filepath}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z57osEb3FIU",
        "outputId": "f146d6ec-488b-4b1e-d894-a9693cac44fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open {downloaded_filepath}, {downloaded_filepath}.zip or {downloaded_filepath}.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import yaml"
      ],
      "metadata": {
        "id": "5X72yTpXnMET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리를 위한 변환 정의 (이미지 크기 조절과 채널 수 조절)\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "# 이미지 데이터를 담은 npy 파일 로드\n",
        "image_data = np.load(x_train_path)\n",
        "# 라벨 정보를 담은 CSV 파일 로드\n",
        "\n",
        "labels_df = pd.read_csv(y_train_path)\n",
        "# 'file_name' 열을 인덱스로 설정\n",
        "labels_df.set_index('file_name', inplace=True)\n",
        "\n",
        "# 이미지 파일과 라벨 정보를 매핑\n",
        "labels = torch.tensor(labels_df['label'].values)\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample, target\n",
        "\n",
        "# 데이터셋과 데이터 로더 생성\n",
        "custom_dataset = CustomDataset(data=image_data, targets=labels, transform=data_transforms)\n",
        "data_loader = DataLoader(dataset=custom_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 간단한 신경망 모델 정의\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(16 * 32 * 32, 64)\n",
        "        self.fc2 = nn.Linear(64, 3)  # 3개의 클래스 (high, mid, low)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = x.view(-1, 16 * 32 * 32)  # Flatten the image tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 모델, 손실 함수, 최적화 함수 초기화\n",
        "model = SimpleNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 모델 학습\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(data_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(data_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('학습이 완료되었습니다.')\n",
        "\n",
        "# 모델 저장\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f'Model saved to {model_save_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQC90TM7mqEm",
        "outputId": "5cacce2f-122e-4030-f617-e119bdc888ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/10], Loss: 1.0639\n",
            "Epoch [2/10], Step [10/10], Loss: 0.8649\n",
            "Epoch [3/10], Step [10/10], Loss: 0.8369\n",
            "Epoch [4/10], Step [10/10], Loss: 0.6025\n",
            "Epoch [5/10], Step [10/10], Loss: 0.8077\n",
            "Epoch [6/10], Step [10/10], Loss: 0.4444\n",
            "Epoch [7/10], Step [10/10], Loss: 0.3404\n",
            "Epoch [8/10], Step [10/10], Loss: 0.2418\n",
            "Epoch [9/10], Step [10/10], Loss: 0.2242\n",
            "Epoch [10/10], Step [10/10], Loss: 0.1504\n",
            "학습이 완료되었습니다.\n",
            "Model saved to /content/model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(16 * 32 * 32, 64)\n",
        "        self.fc2 = nn.Linear(64, 3)  # 3개의 클래스 (high, mid, low)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = x.view(-1, 16 * 32 * 32)  # Flatten the image tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 모델 불러오기\n",
        "model = SimpleNet()\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "# 데이터 전처리 변환 정의 (이미지 크기 조절과 채널 수 조절)\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# 테스트 데이터를 담은 npy 파일 로드\n",
        "test_image_data = np.load(x_test_path)\n",
        "\n",
        "# 테스트 데이터에 대한 추론 수행\n",
        "results = []\n",
        "for image_data in test_image_data:\n",
        "    # 이미지 전처리\n",
        "    image = data_transforms(image_data)\n",
        "    image = image.unsqueeze(0)  # 배치 차원을 추가\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        results.append(predicted.item())\n",
        "\n",
        "# 결과를 DataFrame으로 저장\n",
        "test_df = pd.DataFrame({'file_name': [f'test_{i}.jpg' for i in range(len(results))], 'label': results})\n",
        "\n",
        "# 결과를 CSV 파일로 저장\n",
        "test_df.to_csv(y_tmp_save_path, index=False)\n",
        "\n",
        "print('테스트 결과를 CSV 파일로 저장했습니다.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JOAM1_LmzEI",
        "outputId": "4b464c34-bafe-49b3-fa36-76fb31a6eb95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 결과를 CSV 파일로 저장했습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "\n",
        "# 원본 CSV 파일 열기\n",
        "with open(y_tmp_save_path, 'r', newline='') as infile, \\\n",
        "     open(y_pred_save_path, 'w', newline='') as outfile:\n",
        "\n",
        "    # csv.reader와 csv.writer 객체 생성\n",
        "    reader = csv.reader(infile)\n",
        "    writer = csv.writer(outfile)\n",
        "\n",
        "    # 첫 번째 행(헤더) 건너뛰기\n",
        "    next(reader)\n",
        "\n",
        "    # 각 행에서 첫 번째 열 제거하고 새 파일에 쓰기\n",
        "    for row in reader:\n",
        "        writer.writerow(row[1:])  # 첫 번째 열을 제외한 나머지 열을 쓴다\n",
        "\n",
        "print(f'The modified data has been saved to {y_pred_save_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PztGB73eBisG",
        "outputId": "8663b8cd-54dd-46c8-c766-d6ba67dfa94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The modified data has been saved to /content/y_pred.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#이제 생성된 y_pred 파일을 가지고 채점을 해보세요!"
      ],
      "metadata": {
        "id": "Lol8240RhRFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip -q install gradio==3.45.0"
      ],
      "metadata": {
        "id": "SwY6lKa-lMaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import io"
      ],
      "metadata": {
        "id": "AJPtHGftku9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 불러오는 함수 정의\n",
        "def load_model(model_path):\n",
        "    model = SimpleNet()  # SimpleNet은 이미 정의된 모델 클래스여야 합니다.\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# 이미지를 처리하고 예측을 수행하는 함수 정의\n",
        "def predict_image(image):\n",
        "    # 이미지를 모델이 요구하는 형태로 변환\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    image = Image.open(io.BytesIO(image)).convert('RGB')\n",
        "    image = transform(image)\n",
        "    image = image.unsqueeze(0)  # 모델은 배치로 입력을 받기 때문에 차원 추가 필요\n",
        "\n",
        "    # 예측 수행\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image)\n",
        "        _, predicted_class = torch.max(prediction, 1)\n",
        "        return predicted_class.item()\n",
        "\n",
        "# 모델 경로 설정\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Gradio 인터페이스 생성\n",
        "iface = gr.Interface(\n",
        "    fn=predict_image,  # 실행할 함수\n",
        "    inputs=gr.inputs.Image(shape=(64, 64)),  # 입력 형태\n",
        "    outputs=\"text\",  # 출력 형태\n",
        "    title=\"Image Classifier\",  # 인터페이스 제목\n",
        "    description=\"Upload an image to classify.\"  # 인터페이스 설명\n",
        ")\n",
        "\n",
        "# 인터페이스 실행\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "_oHHPPPVlDze",
        "outputId": "b74cd0a0-ea16-43de-fc8f-44a89a5b5482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-4d346478745a>:32: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  inputs=gr.inputs.Image(shape=(64, 64)),  # 입력 형태\n",
            "<ipython-input-16-4d346478745a>:32: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  inputs=gr.inputs.Image(shape=(64, 64)),  # 입력 형태\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7863, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}