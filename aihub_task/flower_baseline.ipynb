{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 화훼 종류 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[주요 화훼류 품질 데이터](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=71500)\n",
        "<br>[화훼 종류 분류](https://aifactory.space/task/2675/overview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X578npBsdHc",
        "outputId": "482f2dac-2aad-4afc-da32-3948c61dd536"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load data\n",
            "Epoch 1/15\n",
            " 35/504 [=>............................] - ETA: 51s - loss: 2.0900 - accuracy: 0.2598"
          ]
        }
      ],
      "source": [
        "class Dataloader(Sequence):\n",
        "    def __init__(self, base_dataset_path, images, labels, batch_size):\n",
        "        self.base_dataset_path = base_dataset_path\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = np.arange(len(self.labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.labels)/self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        indices = self.indices[idx*self.batch_size : (idx+1)*self.batch_size]\n",
        "        batch_x = [self.images[i] for i in indices]\n",
        "        batch_images = self.get_imagesets(batch_x)\n",
        "        batch_images = batch_images.astype('float32') / 255.0\n",
        "\n",
        "        batch_y = [self.labels[i] for i in indices]\n",
        "        batch_y = to_categorical(batch_y, 9)\n",
        "        return np.array(batch_images), np.array(batch_y)\n",
        "\n",
        "    def get_imagesets(self, path_list):\n",
        "        image_list = []\n",
        "        for image in path_list:\n",
        "            image_path = os.path.join(self.base_dataset_path, image)\n",
        "            image_list.append(cv2.imread(image_path, cv2.IMREAD_GRAYSCALE))\n",
        "        return np.array(image_list)\n",
        "\n",
        "\n",
        "x_train_path = '/content/train'\n",
        "y_train_path = '/content/train.csv'\n",
        "model_save_path = 'Flower_classifier.h5'\n",
        "\n",
        "\n",
        "# 데이터 로드\n",
        "print('load data')\n",
        "df = pd.read_csv(y_train_path)\n",
        "X_train_path, x_test_path, Y_train, y_test = train_test_split(df['image'].values, df['label'].values, train_size=0.8, shuffle=True, stratify=df['label'])\n",
        "\n",
        "train_loader = Dataloader(x_train_path, X_train_path, Y_train, batch_size=64)\n",
        "test_loader = Dataloader(x_train_path, x_test_path, y_test, batch_size=64)\n",
        "\n",
        "# 모델 생성\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dense(9, activation='softmax')  # num_classes에는 종류별 클래스 수를 설정하세요.\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(train_loader, validation_data=test_loader, epochs=10, batch_size=64)#, validation_data=(x_test, y_test))\n",
        "\n",
        "# 모델 저장\n",
        "model.save(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA60R5tstiMj",
        "outputId": "259247fe-014a-40d3-fe52-b73fb723f2df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "270/270 [==============================] - 18s 67ms/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "class Dataloader(Sequence):\n",
        "    def __init__(self, base_dataset_path, images, batch_size):\n",
        "        self.base_dataset_path = base_dataset_path\n",
        "        self.images = images\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = np.arange(len(self.images))\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.images)/self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        indices = self.indices[idx*self.batch_size : (idx+1)*self.batch_size]\n",
        "        batch_x = [self.images[i] for i in indices]\n",
        "        batch_images = self.get_imagesets(batch_x)\n",
        "        batch_images = batch_images.astype('float32') / 255.0\n",
        "        return np.array(batch_images)\n",
        "\n",
        "    def get_imagesets(self, path_list):\n",
        "        image_list = []\n",
        "        for image in path_list:\n",
        "\n",
        "            image_path = os.path.join(self.base_dataset_path, image)\n",
        "            image_list.append(cv2.imread(image_path, cv2.IMREAD_GRAYSCALE))\n",
        "        return np.array(image_list)\n",
        "\n",
        "\n",
        "model_path = 'Flower_classifier.h5'\n",
        "x_test_path = '/content/test'\n",
        "y_pred_save_path = 'Predict.csv'\n",
        "\n",
        "image_names = []\n",
        "labels = []\n",
        "\n",
        "# 모델 로드\n",
        "model = load_model(model_path)\n",
        "\n",
        "df = pd.DataFrame(os.listdir(x_test_path), columns=['image'])\n",
        "inference_dataloader = Dataloader(x_test_path, df['image'].values, 64)\n",
        "\n",
        "# 모델 예측\n",
        "pred = model.predict(inference_dataloader)\n",
        "\n",
        "pred = list(map(np.argmax, pred))\n",
        "df['label'] = pred\n",
        "\n",
        "df.to_csv(y_pred_save_path, index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
