{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0_JIZnnL8x"
      },
      "source": [
        "## 낚시성 기사 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlIMbV4D3gdi"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[낚시성 기사 탐지 데이터](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=71338)\n",
        "<br>[낚시성 기사 분류](https://aifactory.space/task/2663/overview)"
      ],
      "metadata": {
        "id": "3TU52cV425Kf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "e_bzeAVj23tZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y78idJLt3jt1"
      },
      "source": [
        "<b>Load Data</b>\n",
        "<br>구글 드라이브에 저장된 AIHUB Data를 Load하여 압축 해제\n",
        "\n",
        "<br><b>Model Training</b>\n",
        "<br>낚시성 기사 분류 Task에 대한 Training 진행\n",
        "\n",
        "<br><b>Model Test</b>\n",
        "<br>기사의 낚시성 예측값(0, 1)을 pred_y.csv에 저장\n",
        "\n",
        "<br><b>Submission</b>\n",
        "<br>pred_y.csv를 리더보드에 제출하여 채점 진행\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHw791eN39xR"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdfo4i7kMQ3q"
      },
      "source": [
        "### GPU Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL8Q35I6MUPe"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHvtBXiI3T-z"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-VZXMvmpc-S"
      },
      "outputs": [],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJmkxCu6sWWs"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "# !pip install transformers==4.29.2\n",
        "# !pip install safetensors==0.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnZU7_Dwnx2E"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_Cvfxq4pPUG"
      },
      "outputs": [],
      "source": [
        "file_id = \"1q1nO2sUI8j_CgIhs79QIYBzhLCZ1O9En\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu-WgfAKpj7b",
        "outputId": "3f3b168b-3d37-4742-9dec-71216cd4899d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1q1nO2sUI8j_CgIhs79QIYBzhLCZ1O9En\n",
            "To: /content/aihub_clickbait_classification_data.zip\n",
            "100% 57.3M/57.3M [00:00<00:00, 72.4MB/s]\n",
            "Downloaded file: aihub_clickbait_classification_data.zip\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "before_files = set(os.listdir())\n",
        "\n",
        "!gdown {url}\n",
        "\n",
        "after_files = set(os.listdir())\n",
        "\n",
        "downloaded_files = after_files - before_files\n",
        "if downloaded_files:\n",
        "    filename = downloaded_files.pop()\n",
        "    print(f\"Downloaded file: {filename}\")\n",
        "    downloaded_filepath = filename\n",
        "else:\n",
        "    print(\"No file downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUOFvej-pCbV",
        "outputId": "e39e7341-ac17-498b-c295-4fc5ec2431fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  aihub_clickbait_classification_data.zip\n",
            "   creating: aihub_clickbait_classification_data/\n",
            "  inflating: aihub_clickbait_classification_data/test_x.csv  \n",
            "  inflating: aihub_clickbait_classification_data/train.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip {downloaded_filepath}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBAnvYpU3RBj"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380,
          "referenced_widgets": [
            "6325927313d347cda23aa8648306dc4e",
            "9d5dc5863421405fb5d58dd647e07827",
            "e2113828ccce41249ec11aa148934748",
            "0cb622bf3c0b4c529ca4f3fd306c1f44",
            "191829f067714e6291056ae76037c093"
          ]
        },
        "id": "c0-dqNs8nNyc",
        "outputId": "3e3d6edc-761a-4409-e190-583799e36b54"
      },
      "outputs": [
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.007821798324584961,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading pytorch_model.bin",
              "rate": null,
              "total": 272545970,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6325927313d347cda23aa8648306dc4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/273M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0059545040130615234,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading (…)okenizer_config.json",
              "rate": null,
              "total": 375,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d5dc5863421405fb5d58dd647e07827",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.005886554718017578,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading (…)solve/main/vocab.txt",
              "rate": null,
              "total": 248477,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2113828ccce41249ec11aa148934748",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.005810737609863281,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading (…)/main/tokenizer.json",
              "rate": null,
              "total": 751504,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cb622bf3c0b4c529ca4f3fd306c1f44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/752k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.005857944488525391,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading (…)cial_tokens_map.json",
              "rate": null,
              "total": 173,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "191829f067714e6291056ae76037c093",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1:   0%|          | 0/908 [00:00<?, ?it/s]/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Epoch 1: 100%|██████████| 908/908 [03:08<00:00,  4.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Time 00:03:08    Train Loss: 587.3607    Train Accuracy: 0.6042\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 908/908 [03:08<00:00,  4.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Time 00:03:08    Train Loss: 518.0104    Train Accuracy: 0.6952\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 908/908 [03:08<00:00,  4.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Time 00:03:08    Train Loss: 467.4494    Train Accuracy: 0.7391\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 908/908 [03:08<00:00,  4.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Time 00:03:08    Train Loss: 413.0478    Train Accuracy: 0.7814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 908/908 [03:08<00:00,  4.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Time 00:03:08    Train Loss: 353.5881    Train Accuracy: 0.8232\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 908/908 [03:08<00:00,  4.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Time 00:03:08    Train Loss: 295.2479    Train Accuracy: 0.8592\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 908/908 [03:09<00:00,  4.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Time 00:03:09    Train Loss: 237.5199    Train Accuracy: 0.8918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 908/908 [03:09<00:00,  4.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Time 00:03:09    Train Loss: 191.0845    Train Accuracy: 0.9149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 908/908 [03:08<00:00,  4.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Time 00:03:08    Train Loss: 153.3742    Train Accuracy: 0.9338\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 908/908 [03:08<00:00,  4.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Time 00:03:08    Train Loss: 121.9972    Train Accuracy: 0.9477\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import datetime\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from transformers import AutoTokenizer, RobertaForSequenceClassification, AdamW\n",
        "\n",
        "\n",
        "def main(pretrained_model, train_data_path,  model_save_folder, epoch, batch_size):\n",
        "\n",
        "    # os.mkdir(model_save_folder)\n",
        "\n",
        "    class ClickbaitDetectionDataset(Dataset):\n",
        "\n",
        "        def __init__(self, dataset):\n",
        "            self.dataset = dataset\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.dataset)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            row = self.dataset.iloc[idx, 0:2].values\n",
        "            text = row[0]\n",
        "            y = row[1]\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                text,\n",
        "                return_tensors='pt',\n",
        "                truncation=True,\n",
        "                max_length=256,\n",
        "                pad_to_max_length=True,\n",
        "                add_special_tokens=True\n",
        "                )\n",
        "\n",
        "            input_ids = inputs['input_ids'][0]\n",
        "            attention_mask = inputs['attention_mask'][0]\n",
        "\n",
        "            return input_ids, attention_mask, y\n",
        "\n",
        "    if torch.cuda.is_available() == True:\n",
        "        device = torch.device(\"cuda:0\")\n",
        "        model = RobertaForSequenceClassification.from_pretrained(pretrained_model).to(device)\n",
        "    elif torch.cuda.is_available() == False:\n",
        "        model = RobertaForSequenceClassification.from_pretrained(pretrained_model)\n",
        "\n",
        "    train_data = pd.read_csv(train_data_path)\n",
        "    train_dataset = ClickbaitDetectionDataset(train_data)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    train_time = []\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "\n",
        "    for i in range(epoch):\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        batches = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        with tqdm(train_loader) as pbar:\n",
        "            pbar.set_description(\"Epoch \" + str(i + 1))\n",
        "            for input_ids_batch, attention_masks_batch, y_batch in pbar:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                if torch.cuda.is_available() == True:\n",
        "                    y_batch = y_batch.to(device)\n",
        "                    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
        "\n",
        "                elif torch.cuda.is_available() == False:\n",
        "                    y_batch = y_batch\n",
        "                    y_pred = model(input_ids_batch, attention_mask=attention_masks_batch)[0]\n",
        "\n",
        "\n",
        "                one_loss = F.cross_entropy(y_pred, y_batch)\n",
        "                one_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += one_loss.item()\n",
        "\n",
        "                _, predicted = torch.max(y_pred, 1)\n",
        "                correct += (predicted == y_batch).sum()\n",
        "                total += len(y_batch)\n",
        "\n",
        "                batches += 1\n",
        "                # if batches % 100 == 0:\n",
        "                # print(\"Batch Loss:\", total_loss, \"Accuracy:\", correct.float() / total)\n",
        "\n",
        "                elapsed = pbar.format_dict['elapsed']\n",
        "                elapsed_str = pbar.format_interval(elapsed)\n",
        "\n",
        "\n",
        "        if len(elapsed_str) == 5:\n",
        "            elapsed_str = \"00:\" + elapsed_str\n",
        "        elapsed_str = str(datetime.datetime.strptime(elapsed_str, '%H:%M:%S').time())\n",
        "\n",
        "        pbar.close()\n",
        "        train_time.append(elapsed_str)\n",
        "        total_loss = round(total_loss, 4)\n",
        "        train_loss.append(total_loss)\n",
        "        accuracy = round((correct.float() / total).item(), 4)\n",
        "        train_accuracy.append(accuracy)\n",
        "        print(\"Train Time\",  elapsed_str, \"  \", \"Train Loss:\", total_loss,  \"  \",  \"Train Accuracy:\", accuracy)\n",
        "\n",
        "        torch.save(model.state_dict(), model_save_folder + \"clickbait_classifcation_model_\" + str(i + 1) + \".bin\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    pretrained_model = 'klue/roberta-small'\n",
        "    train_data_path = \"aihub_clickbait_classification_data/train.csv\"\n",
        "    model_save_folder =  \"./model/\"\n",
        "    epoch = 10\n",
        "    batch_size = 64\n",
        "    main(pretrained_model, train_data_path, model_save_folder, epoch, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpnHvfP73aOW"
      },
      "source": [
        "### Model Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHv7-rAfvxHY",
        "outputId": "2042f1be-369c-4e7d-b485-c01c4dbb2a90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "100%|██████████| 10/10 [00:01<00:00,  9.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Time 00:00:01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import datetime\n",
        "import regex as re\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from transformers import AutoTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "\n",
        "def main(pretrained_model, test_data_path, load_model_path, predction_csv_path, epoch, batch_size):\n",
        "\n",
        "    class ClickbaitDetectionDataset(Dataset):\n",
        "\n",
        "        def __init__(self, dataset):\n",
        "            self.dataset = dataset\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.dataset)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            row = self.dataset.iloc[idx, 0:1].values\n",
        "            text = row[0]\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                text,\n",
        "                return_tensors='pt',\n",
        "                truncation=True,\n",
        "                max_length=256,\n",
        "                pad_to_max_length=True,\n",
        "                add_special_tokens=True\n",
        "                )\n",
        "\n",
        "            input_ids = inputs['input_ids'][0]\n",
        "            attention_mask = inputs['attention_mask'][0]\n",
        "\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "    if torch.cuda.is_available() == True:\n",
        "        device = torch.device(\"cuda\")\n",
        "        model = RobertaForSequenceClassification.from_pretrained(pretrained_model).to(device)\n",
        "    elif torch.cuda.is_available() == False:\n",
        "        model = RobertaForSequenceClassification.from_pretrained(pretrained_model)\n",
        "\n",
        "    checkpoint = torch.load(os.path.join(load_model_path, \"clickbait_classifcation_model_\" + str(epoch) + \".bin\"))\n",
        "    model.load_state_dict(checkpoint)\n",
        "    model.eval()\n",
        "\n",
        "    test_data = pd.read_csv(test_data_path)\n",
        "    test_dataset = ClickbaitDetectionDataset(test_data)\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    predicted_list = []\n",
        "\n",
        "    with tqdm(test_loader) as pbar:\n",
        "        for input_ids_batch, attention_masks_batch in pbar:\n",
        "\n",
        "            if torch.cuda.is_available() == True:\n",
        "                y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
        "\n",
        "            elif torch.cuda.is_available() == False:\n",
        "                y_pred = model(input_ids_batch, attention_mask=attention_masks_batch)[0]\n",
        "\n",
        "            _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "            for prediction in predicted.tolist():\n",
        "                predicted_list.append(prediction)\n",
        "\n",
        "            elapsed = pbar.format_dict['elapsed']\n",
        "            elapsed_str = pbar.format_interval(elapsed)\n",
        "\n",
        "\n",
        "        if len(elapsed_str) == 5:\n",
        "            elapsed_str = \"00:\" + elapsed_str\n",
        "        elapsed_str = str(datetime.datetime.strptime(elapsed_str, '%H:%M:%S').time())\n",
        "\n",
        "        pbar.close()\n",
        "        print(\"Test Time\",  elapsed_str)\n",
        "\n",
        "    y_pred_csv = pd.DataFrame({\"Prediction\": predicted_list})\n",
        "    y_pred_csv.to_csv(predction_csv_path, index=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    pretrained_model = 'klue/roberta-small'\n",
        "    test_data_path = \"aihub_clickbait_classification_data/test_x.csv\"\n",
        "    load_model_path =  \"./model/\"\n",
        "    predction_csv_path = \"log/pred_y.csv\"\n",
        "    epoch = 5\n",
        "    batch_size = 64\n",
        "    main(pretrained_model, test_data_path, load_model_path, predction_csv_path, epoch, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradio"
      ],
      "metadata": {
        "id": "xzrlK8002J44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install --upgrade typing-extensions"
      ],
      "metadata": {
        "id": "kY-OYle72I0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "def main(text):\n",
        "\n",
        "    class ClickbaitDetectionDataset(Dataset):\n",
        "\n",
        "        def __init__(self, dataset):\n",
        "            self.dataset = dataset\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.dataset)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            row = self.dataset.iloc[idx, 0:2].values\n",
        "            text = row[0]\n",
        "            y = row[1]\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                text,\n",
        "                return_tensors='pt',\n",
        "                truncation=True,\n",
        "                max_length=256,\n",
        "                pad_to_max_length=True,\n",
        "                add_special_tokens=True\n",
        "                )\n",
        "\n",
        "            input_ids = inputs['input_ids'][0]\n",
        "            attention_mask = inputs['attention_mask'][0]\n",
        "\n",
        "            return input_ids, attention_mask, y\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-base\").to(device)\n",
        "    checkpoint = torch.load(\"model/clickbait_classifcation_model_5.bin\")\n",
        "    model.load_state_dict(checkpoint)\n",
        "    model.eval()\n",
        "\n",
        "    test_data = pd.DataFrame({\"Text\":[text]})\n",
        "    test_dataset = ClickbaitDetectionDataset(test_data)\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids_batch, attention_masks_batch in test_loader:\n",
        "            # optimizer.zero_grad()\n",
        "\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
        "            _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "            if predicted.item() == 0:\n",
        "                classfication_result = \"It's Not Clickbait Article\"\n",
        "            elif predicted.item() == 1:\n",
        "                classfication_result =  \"It's Clickbait Article\"\n",
        "\n",
        "    return classfication_result\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    demo = gr.Interface(fn=main, inputs=\"text\", outputs=\"text\")\n",
        "    demo.launch()\n",
        "    # demo.launch( share = True , debug = True)"
      ],
      "metadata": {
        "id": "-KczEwNg2NHX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}