{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 화질변화 영상 데이터 - 초해상화 Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[화질 변환 영상 데이터](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=71540)\n",
        "<br>[화질변화 영상 데이터 - 초해상화 Task](https://aifactory.space/task/2644/overview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbhY46jmi1TM",
        "outputId": "f0199eb4-c5f6-4908-b1ad-65978baaaecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORFJOYhWi-vN",
        "outputId": "4bfeef64-ea24-408a-8c44-4d932319e24d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1crEpiw-fbJyhsZAu-84Cp7_rj2SAsrnJ\n",
            "To: /content/LR.zip\n",
            "100% 405M/405M [00:05<00:00, 69.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PClJ9OrPIZ-TX8qm8MX3Su2F4EMjfza1\n",
            "To: /content/HR.zip\n",
            "100% 1.13G/1.13G [00:15<00:00, 71.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uJyoEsbF-DVpsMqVpuMUwAXWPP0rsWkl\n",
            "To: /content/Test_LR.zip\n",
            "100% 11.1M/11.1M [00:00<00:00, 44.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "#Train LR\n",
        "file_id='1crEpiw-fbJyhsZAu-84Cp7_rj2SAsrnJ' #zip 버전\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "!gdown {url}\n",
        "\n",
        "#Train HR\n",
        "file_id='1PClJ9OrPIZ-TX8qm8MX3Su2F4EMjfza1' #zip 버전\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "!gdown {url}\n",
        "\n",
        "#Test LR\n",
        "file_id='1uJyoEsbF-DVpsMqVpuMUwAXWPP0rsWkl' #zip 버전\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "!gdown {url}\n",
        "\n",
        "import os\n",
        "os.mkdir('/content/weight')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz0fp3QbKDe6",
        "outputId": "45868f69-a0e2-471c-c7a7-3545e3798d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "espZgD2WhGVw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import ToTensor, Resize\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SIjyf0DiQKL"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Train LR 경로 입력\n",
        "lr_file='/content/LR.zip'\n",
        "#Train HR 경로 입력\n",
        "hr_file='/content/HR.zip'\n",
        "#weight 저장\n",
        "model_weight_path='/content/weight'\n",
        "\n",
        "lr_folder=lr_file[:-4]\n",
        "hr_folder=hr_file[:-4]\n",
        "\n",
        "if not os.path.exists(lr_folder):\n",
        "    os.system('unzip {} -d {}'.format(lr_file,lr_folder))\n",
        "\n",
        "if not os.path.exists(hr_folder):\n",
        "    os.system('unzip {} -d {}'.format(hr_file,hr_folder))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3yMzRamiSAf"
      },
      "outputs": [],
      "source": [
        "# SRCNN 모델 정의\n",
        "class SRCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SRCNN, self).__init__()\n",
        "        self.layer1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
        "        self.layer2 = nn.Conv2d(64, 32, kernel_size=1)\n",
        "        self.layer3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyx3Ot3siUam"
      },
      "outputs": [],
      "source": [
        "# 커스텀 데이터셋 정의\n",
        "class SRDataset(Dataset):\n",
        "    def __init__(self, lr_folder, hr_folder, transform=None):\n",
        "        self.lr_folder = lr_folder\n",
        "        self.hr_folder = hr_folder\n",
        "        self.transform = transform\n",
        "        self.filenames = os.listdir(lr_folder)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lr_img = Image.open(os.path.join(self.lr_folder, self.filenames[idx]))\n",
        "        hr_img = Image.open(os.path.join(self.hr_folder, self.filenames[idx].replace('H01','H03')))\n",
        "\n",
        "        if self.transform:\n",
        "            lr_img = self.transform(lr_img)\n",
        "            lr_img = ToTensor()(lr_img)\n",
        "            hr_img = ToTensor()(hr_img)\n",
        "        return lr_img, hr_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBGcaIqxiWwu"
      },
      "outputs": [],
      "source": [
        "# 데이터 로드\n",
        "# SRCNN 모델은 내부적으로 upscale을 진행하지 않기 때문에 input 이미지를 키워서 입력\n",
        "transform = Resize((1440, 2560), interpolation=Image.BICUBIC)\n",
        "#터미널 창에 lr_folder와 hr_folder를 함께 입력으로 제공\n",
        "train_dataset = SRDataset(lr_folder=lr_folder, hr_folder=hr_folder, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4pneRRtiZce"
      },
      "outputs": [],
      "source": [
        "# 모델, 손실 함수, 옵티마이저 정의\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SRCNN().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhhnVeYMYKh_"
      },
      "outputs": [],
      "source": [
        "def psnr(img1, img2, border=0):\n",
        "    h, w = img1.shape[:2]\n",
        "    img1 = img1[border:h-border, border:w-border]\n",
        "    img2 = img2[border:h-border, border:w-border]\n",
        "    mse = torch.mean((img1 - img2) ** 2)\n",
        "    score =  torch.log10(255.0 / torch.sqrt(mse))\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_zFDz87icE2",
        "outputId": "b9c1af0c-9e74-46cc-cbe1-90a205faa516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1], Batch [1/839], Loss: 0.2387\n",
            "Epoch [1/1], Batch [2/839], Loss: 0.1987\n",
            "Epoch [1/1], Batch [3/839], Loss: 0.2302\n",
            "Epoch [1/1], Batch [4/839], Loss: 0.1811\n",
            "Epoch [1/1], Batch [5/839], Loss: 0.1602\n",
            "Epoch [1/1], Batch [6/839], Loss: 0.2172\n",
            "Epoch [1/1], Batch [7/839], Loss: 0.1778\n",
            "Epoch [1/1], Batch [8/839], Loss: 0.1715\n",
            "Epoch [1/1], Batch [9/839], Loss: 0.2096\n",
            "Epoch [1/1], Batch [10/839], Loss: 0.2044\n",
            "Epoch [1/1], Batch [11/839], Loss: 0.1840\n",
            "Epoch [1/1], Batch [12/839], Loss: 0.1521\n",
            "Epoch [1/1], Batch [13/839], Loss: 0.1180\n",
            "Epoch [1/1], Batch [14/839], Loss: 0.1497\n",
            "Epoch [1/1], Batch [15/839], Loss: 0.1308\n",
            "Epoch [1/1], Batch [16/839], Loss: 0.1313\n",
            "Epoch [1/1], Batch [17/839], Loss: 0.1462\n",
            "Epoch [1/1], Batch [18/839], Loss: 0.1096\n",
            "Epoch [1/1], Batch [19/839], Loss: 0.0767\n",
            "Epoch [1/1], Batch [20/839], Loss: 0.1139\n",
            "Epoch [1/1], Batch [21/839], Loss: 0.0586\n",
            "Epoch [1/1], Batch [22/839], Loss: 0.0656\n",
            "Epoch [1/1], Batch [23/839], Loss: 0.0603\n",
            "Epoch [1/1], Batch [24/839], Loss: 0.0494\n",
            "Epoch [1/1], Batch [25/839], Loss: 0.0364\n",
            "Epoch [1/1], Batch [26/839], Loss: 0.0561\n",
            "Epoch [1/1], Batch [27/839], Loss: 0.0415\n",
            "Epoch [1/1], Batch [28/839], Loss: 0.0212\n",
            "Epoch [1/1], Batch [29/839], Loss: 0.0224\n",
            "Epoch [1/1], Batch [30/839], Loss: 0.0241\n",
            "Epoch [1/1], Batch [31/839], Loss: 0.0148\n",
            "Epoch [1/1], Batch [32/839], Loss: 0.0146\n",
            "Epoch [1/1], Batch [33/839], Loss: 0.0108\n",
            "Epoch [1/1], Batch [34/839], Loss: 0.0216\n",
            "Epoch [1/1], Batch [35/839], Loss: 0.0151\n",
            "Epoch [1/1], Batch [36/839], Loss: 0.0126\n",
            "Epoch [1/1], Batch [37/839], Loss: 0.0122\n",
            "Epoch [1/1], Batch [38/839], Loss: 0.0143\n",
            "Epoch [1/1], Batch [39/839], Loss: 0.0136\n",
            "Epoch [1/1], Batch [40/839], Loss: 0.0173\n",
            "Epoch [1/1], Batch [41/839], Loss: 0.0101\n",
            "Epoch [1/1], Batch [42/839], Loss: 0.0171\n",
            "Epoch [1/1], Batch [43/839], Loss: 0.0094\n",
            "Epoch [1/1], Batch [44/839], Loss: 0.0176\n",
            "Epoch [1/1], Batch [45/839], Loss: 0.0058\n",
            "Epoch [1/1], Batch [46/839], Loss: 0.0069\n",
            "Epoch [1/1], Batch [47/839], Loss: 0.0171\n",
            "Epoch [1/1], Batch [48/839], Loss: 0.0055\n",
            "Epoch [1/1], Batch [49/839], Loss: 0.0033\n",
            "Epoch [1/1], Batch [50/839], Loss: 0.0092\n",
            "Epoch [1/1], Batch [51/839], Loss: 0.0090\n",
            "Epoch [1/1], Batch [52/839], Loss: 0.0100\n",
            "Epoch [1/1], Batch [53/839], Loss: 0.0047\n",
            "Epoch [1/1], Batch [54/839], Loss: 0.0044\n",
            "Epoch [1/1], Batch [55/839], Loss: 0.0107\n",
            "Epoch [1/1], Batch [56/839], Loss: 0.0109\n",
            "Epoch [1/1], Batch [57/839], Loss: 0.0096\n",
            "Epoch [1/1], Batch [58/839], Loss: 0.0055\n",
            "Epoch [1/1], Batch [59/839], Loss: 0.0063\n",
            "Epoch [1/1], Batch [60/839], Loss: 0.0056\n",
            "Epoch [1/1], Batch [61/839], Loss: 0.0088\n",
            "Epoch [1/1], Batch [62/839], Loss: 0.0112\n",
            "Epoch [1/1], Batch [63/839], Loss: 0.0058\n",
            "Epoch [1/1], Batch [64/839], Loss: 0.0053\n",
            "Epoch [1/1], Batch [65/839], Loss: 0.0049\n",
            "Epoch [1/1], Batch [66/839], Loss: 0.0040\n",
            "Epoch [1/1], Batch [67/839], Loss: 0.0056\n",
            "Epoch [1/1], Batch [68/839], Loss: 0.0166\n",
            "Epoch [1/1], Batch [69/839], Loss: 0.0061\n",
            "Epoch [1/1], Batch [70/839], Loss: 0.0076\n",
            "Epoch [1/1], Batch [71/839], Loss: 0.0048\n",
            "Epoch [1/1], Batch [72/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [73/839], Loss: 0.0036\n",
            "Epoch [1/1], Batch [74/839], Loss: 0.0374\n",
            "Epoch [1/1], Batch [75/839], Loss: 0.0032\n",
            "Epoch [1/1], Batch [76/839], Loss: 0.0080\n",
            "Epoch [1/1], Batch [77/839], Loss: 0.0027\n",
            "Epoch [1/1], Batch [78/839], Loss: 0.0035\n",
            "Epoch [1/1], Batch [79/839], Loss: 0.0118\n",
            "Epoch [1/1], Batch [80/839], Loss: 0.0051\n",
            "Epoch [1/1], Batch [81/839], Loss: 0.0086\n",
            "Epoch [1/1], Batch [82/839], Loss: 0.0124\n",
            "Epoch [1/1], Batch [83/839], Loss: 0.0049\n",
            "Epoch [1/1], Batch [84/839], Loss: 0.0043\n",
            "Epoch [1/1], Batch [85/839], Loss: 0.0068\n",
            "Epoch [1/1], Batch [86/839], Loss: 0.0113\n",
            "Epoch [1/1], Batch [87/839], Loss: 0.0080\n",
            "Epoch [1/1], Batch [88/839], Loss: 0.0031\n",
            "Epoch [1/1], Batch [89/839], Loss: 0.0032\n",
            "Epoch [1/1], Batch [90/839], Loss: 0.0061\n",
            "Epoch [1/1], Batch [91/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [92/839], Loss: 0.0044\n",
            "Epoch [1/1], Batch [93/839], Loss: 0.0049\n",
            "Epoch [1/1], Batch [94/839], Loss: 0.0049\n",
            "Epoch [1/1], Batch [95/839], Loss: 0.0075\n",
            "Epoch [1/1], Batch [96/839], Loss: 0.0025\n",
            "Epoch [1/1], Batch [97/839], Loss: 0.0108\n",
            "Epoch [1/1], Batch [98/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [99/839], Loss: 0.0116\n",
            "Epoch [1/1], Batch [100/839], Loss: 0.0034\n",
            "Epoch [1/1], Batch [101/839], Loss: 0.0079\n",
            "Epoch [1/1], Batch [102/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [103/839], Loss: 0.0070\n",
            "Epoch [1/1], Batch [104/839], Loss: 0.0094\n",
            "Epoch [1/1], Batch [105/839], Loss: 0.0103\n",
            "Epoch [1/1], Batch [106/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [107/839], Loss: 0.0082\n",
            "Epoch [1/1], Batch [108/839], Loss: 0.0043\n",
            "Epoch [1/1], Batch [109/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [110/839], Loss: 0.0070\n",
            "Epoch [1/1], Batch [111/839], Loss: 0.0039\n",
            "Epoch [1/1], Batch [112/839], Loss: 0.0030\n",
            "Epoch [1/1], Batch [113/839], Loss: 0.0027\n",
            "Epoch [1/1], Batch [114/839], Loss: 0.0066\n",
            "Epoch [1/1], Batch [115/839], Loss: 0.0027\n",
            "Epoch [1/1], Batch [116/839], Loss: 0.0049\n",
            "Epoch [1/1], Batch [117/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [118/839], Loss: 0.0062\n",
            "Epoch [1/1], Batch [119/839], Loss: 0.0044\n",
            "Epoch [1/1], Batch [120/839], Loss: 0.0026\n",
            "Epoch [1/1], Batch [121/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [122/839], Loss: 0.0051\n",
            "Epoch [1/1], Batch [123/839], Loss: 0.0035\n",
            "Epoch [1/1], Batch [124/839], Loss: 0.0042\n",
            "Epoch [1/1], Batch [125/839], Loss: 0.0026\n",
            "Epoch [1/1], Batch [126/839], Loss: 0.0026\n",
            "Epoch [1/1], Batch [127/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [128/839], Loss: 0.0033\n",
            "Epoch [1/1], Batch [129/839], Loss: 0.0027\n",
            "Epoch [1/1], Batch [130/839], Loss: 0.0030\n",
            "Epoch [1/1], Batch [131/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [132/839], Loss: 0.0084\n",
            "Epoch [1/1], Batch [133/839], Loss: 0.0027\n",
            "Epoch [1/1], Batch [134/839], Loss: 0.0023\n",
            "Epoch [1/1], Batch [135/839], Loss: 0.0043\n",
            "Epoch [1/1], Batch [136/839], Loss: 0.0051\n",
            "Epoch [1/1], Batch [137/839], Loss: 0.0045\n",
            "Epoch [1/1], Batch [138/839], Loss: 0.0031\n",
            "Epoch [1/1], Batch [139/839], Loss: 0.0080\n",
            "Epoch [1/1], Batch [140/839], Loss: 0.0032\n",
            "Epoch [1/1], Batch [141/839], Loss: 0.0052\n",
            "Epoch [1/1], Batch [142/839], Loss: 0.0023\n",
            "Epoch [1/1], Batch [143/839], Loss: 0.0027\n",
            "Epoch [1/1], Batch [144/839], Loss: 0.0027\n",
            "Epoch [1/1], Batch [145/839], Loss: 0.0048\n",
            "Epoch [1/1], Batch [146/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [147/839], Loss: 0.0041\n",
            "Epoch [1/1], Batch [148/839], Loss: 0.0037\n",
            "Epoch [1/1], Batch [149/839], Loss: 0.0026\n",
            "Epoch [1/1], Batch [150/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [151/839], Loss: 0.0041\n",
            "Epoch [1/1], Batch [152/839], Loss: 0.0025\n",
            "Epoch [1/1], Batch [153/839], Loss: 0.0017\n",
            "Epoch [1/1], Batch [154/839], Loss: 0.0025\n",
            "Epoch [1/1], Batch [155/839], Loss: 0.0028\n",
            "Epoch [1/1], Batch [156/839], Loss: 0.0043\n",
            "Epoch [1/1], Batch [157/839], Loss: 0.0043\n",
            "Epoch [1/1], Batch [158/839], Loss: 0.0052\n",
            "Epoch [1/1], Batch [159/839], Loss: 0.0059\n",
            "Epoch [1/1], Batch [160/839], Loss: 0.0025\n",
            "Epoch [1/1], Batch [161/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [162/839], Loss: 0.0046\n",
            "Epoch [1/1], Batch [163/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [164/839], Loss: 0.0038\n",
            "Epoch [1/1], Batch [165/839], Loss: 0.0034\n",
            "Epoch [1/1], Batch [166/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [167/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [168/839], Loss: 0.0031\n",
            "Epoch [1/1], Batch [169/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [170/839], Loss: 0.0058\n",
            "Epoch [1/1], Batch [171/839], Loss: 0.0029\n",
            "Epoch [1/1], Batch [172/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [173/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [174/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [175/839], Loss: 0.0030\n",
            "Epoch [1/1], Batch [176/839], Loss: 0.0027\n",
            "Epoch [1/1], Batch [177/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [178/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [179/839], Loss: 0.0031\n",
            "Epoch [1/1], Batch [180/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [181/839], Loss: 0.0023\n",
            "Epoch [1/1], Batch [182/839], Loss: 0.0022\n",
            "Epoch [1/1], Batch [183/839], Loss: 0.0034\n",
            "Epoch [1/1], Batch [184/839], Loss: 0.0023\n",
            "Epoch [1/1], Batch [185/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [186/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [187/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [188/839], Loss: 0.0053\n",
            "Epoch [1/1], Batch [189/839], Loss: 0.0036\n",
            "Epoch [1/1], Batch [190/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [191/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [192/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [193/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [194/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [195/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [196/839], Loss: 0.0040\n",
            "Epoch [1/1], Batch [197/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [198/839], Loss: 0.0026\n",
            "Epoch [1/1], Batch [199/839], Loss: 0.0033\n",
            "Epoch [1/1], Batch [200/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [201/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [202/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [203/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [204/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [205/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [206/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [207/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [208/839], Loss: 0.0029\n",
            "Epoch [1/1], Batch [209/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [210/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [211/839], Loss: 0.0022\n",
            "Epoch [1/1], Batch [212/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [213/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [214/839], Loss: 0.0027\n",
            "Epoch [1/1], Batch [215/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [216/839], Loss: 0.0041\n",
            "Epoch [1/1], Batch [217/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [218/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [219/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [220/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [221/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [222/839], Loss: 0.0025\n",
            "Epoch [1/1], Batch [223/839], Loss: 0.0023\n",
            "Epoch [1/1], Batch [224/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [225/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [226/839], Loss: 0.0048\n",
            "Epoch [1/1], Batch [227/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [228/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [229/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [230/839], Loss: 0.0044\n",
            "Epoch [1/1], Batch [231/839], Loss: 0.0017\n",
            "Epoch [1/1], Batch [232/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [233/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [234/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [235/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [236/839], Loss: 0.0022\n",
            "Epoch [1/1], Batch [237/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [238/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [239/839], Loss: 0.0039\n",
            "Epoch [1/1], Batch [240/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [241/839], Loss: 0.0044\n",
            "Epoch [1/1], Batch [242/839], Loss: 0.0047\n",
            "Epoch [1/1], Batch [243/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [244/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [245/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [246/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [247/839], Loss: 0.0023\n",
            "Epoch [1/1], Batch [248/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [249/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [250/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [251/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [252/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [253/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [254/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [255/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [256/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [257/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [258/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [259/839], Loss: 0.0030\n",
            "Epoch [1/1], Batch [260/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [261/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [262/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [263/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [264/839], Loss: 0.0028\n",
            "Epoch [1/1], Batch [265/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [266/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [267/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [268/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [269/839], Loss: 0.0034\n",
            "Epoch [1/1], Batch [270/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [271/839], Loss: 0.0023\n",
            "Epoch [1/1], Batch [272/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [273/839], Loss: 0.0052\n",
            "Epoch [1/1], Batch [274/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [275/839], Loss: 0.0017\n",
            "Epoch [1/1], Batch [276/839], Loss: 0.0030\n",
            "Epoch [1/1], Batch [277/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [278/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [279/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [280/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [281/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [282/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [283/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [284/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [285/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [286/839], Loss: 0.0017\n",
            "Epoch [1/1], Batch [287/839], Loss: 0.0023\n",
            "Epoch [1/1], Batch [288/839], Loss: 0.0030\n",
            "Epoch [1/1], Batch [289/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [290/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [291/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [292/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [293/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [294/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [295/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [296/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [297/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [298/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [299/839], Loss: 0.0022\n",
            "Epoch [1/1], Batch [300/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [301/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [302/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [303/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [304/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [305/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [306/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [307/839], Loss: 0.0029\n",
            "Epoch [1/1], Batch [308/839], Loss: 0.0035\n",
            "Epoch [1/1], Batch [309/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [310/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [311/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [312/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [313/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [314/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [315/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [316/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [317/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [318/839], Loss: 0.0017\n",
            "Epoch [1/1], Batch [319/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [320/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [321/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [322/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [323/839], Loss: 0.0017\n",
            "Epoch [1/1], Batch [324/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [325/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [326/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [327/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [328/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [329/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [330/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [331/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [332/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [333/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [334/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [335/839], Loss: 0.0023\n",
            "Epoch [1/1], Batch [336/839], Loss: 0.0022\n",
            "Epoch [1/1], Batch [337/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [338/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [339/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [340/839], Loss: 0.0043\n",
            "Epoch [1/1], Batch [341/839], Loss: 0.0030\n",
            "Epoch [1/1], Batch [342/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [343/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [344/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [345/839], Loss: 0.0025\n",
            "Epoch [1/1], Batch [346/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [347/839], Loss: 0.0027\n",
            "Epoch [1/1], Batch [348/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [349/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [350/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [351/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [352/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [353/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [354/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [355/839], Loss: 0.0036\n",
            "Epoch [1/1], Batch [356/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [357/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [358/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [359/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [360/839], Loss: 0.0026\n",
            "Epoch [1/1], Batch [361/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [362/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [363/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [364/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [365/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [366/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [367/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [368/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [369/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [370/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [371/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [372/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [373/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [374/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [375/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [376/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [377/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [378/839], Loss: 0.0022\n",
            "Epoch [1/1], Batch [379/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [380/839], Loss: 0.0027\n",
            "Epoch [1/1], Batch [381/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [382/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [383/839], Loss: 0.0023\n",
            "Epoch [1/1], Batch [384/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [385/839], Loss: 0.0017\n",
            "Epoch [1/1], Batch [386/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [387/839], Loss: 0.0017\n",
            "Epoch [1/1], Batch [388/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [389/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [390/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [391/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [392/839], Loss: 0.0026\n",
            "Epoch [1/1], Batch [393/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [394/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [395/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [396/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [397/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [398/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [399/839], Loss: 0.0025\n",
            "Epoch [1/1], Batch [400/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [401/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [402/839], Loss: 0.0023\n",
            "Epoch [1/1], Batch [403/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [404/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [405/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [406/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [407/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [408/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [409/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [410/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [411/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [412/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [413/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [414/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [415/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [416/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [417/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [418/839], Loss: 0.0043\n",
            "Epoch [1/1], Batch [419/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [420/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [421/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [422/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [423/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [424/839], Loss: 0.0029\n",
            "Epoch [1/1], Batch [425/839], Loss: 0.0023\n",
            "Epoch [1/1], Batch [426/839], Loss: 0.0030\n",
            "Epoch [1/1], Batch [427/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [428/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [429/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [430/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [431/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [432/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [433/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [434/839], Loss: 0.0025\n",
            "Epoch [1/1], Batch [435/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [436/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [437/839], Loss: 0.0029\n",
            "Epoch [1/1], Batch [438/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [439/839], Loss: 0.0027\n",
            "Epoch [1/1], Batch [440/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [441/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [442/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [443/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [444/839], Loss: 0.0030\n",
            "Epoch [1/1], Batch [445/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [446/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [447/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [448/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [449/839], Loss: 0.0017\n",
            "Epoch [1/1], Batch [450/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [451/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [452/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [453/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [454/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [455/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [456/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [457/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [458/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [459/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [460/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [461/839], Loss: 0.0017\n",
            "Epoch [1/1], Batch [462/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [463/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [464/839], Loss: 0.0038\n",
            "Epoch [1/1], Batch [465/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [466/839], Loss: 0.0029\n",
            "Epoch [1/1], Batch [467/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [468/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [469/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [470/839], Loss: 0.0050\n",
            "Epoch [1/1], Batch [471/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [472/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [473/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [474/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [475/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [476/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [477/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [478/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [479/839], Loss: 0.0030\n",
            "Epoch [1/1], Batch [480/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [481/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [482/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [483/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [484/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [485/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [486/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [487/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [488/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [489/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [490/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [491/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [492/839], Loss: 0.0023\n",
            "Epoch [1/1], Batch [493/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [494/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [495/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [496/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [497/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [498/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [499/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [500/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [501/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [502/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [503/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [504/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [505/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [506/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [507/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [508/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [509/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [510/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [511/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [512/839], Loss: 0.0017\n",
            "Epoch [1/1], Batch [513/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [514/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [515/839], Loss: 0.0037\n",
            "Epoch [1/1], Batch [516/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [517/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [518/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [519/839], Loss: 0.0034\n",
            "Epoch [1/1], Batch [520/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [521/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [522/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [523/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [524/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [525/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [526/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [527/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [528/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [529/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [530/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [531/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [532/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [533/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [534/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [535/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [536/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [537/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [538/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [539/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [540/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [541/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [542/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [543/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [544/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [545/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [546/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [547/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [548/839], Loss: 0.0002\n",
            "Epoch [1/1], Batch [549/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [550/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [551/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [552/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [553/839], Loss: 0.0041\n",
            "Epoch [1/1], Batch [554/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [555/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [556/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [557/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [558/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [559/839], Loss: 0.0028\n",
            "Epoch [1/1], Batch [560/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [561/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [562/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [563/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [564/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [565/839], Loss: 0.0002\n",
            "Epoch [1/1], Batch [566/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [567/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [568/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [569/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [570/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [571/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [572/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [573/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [574/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [575/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [576/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [577/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [578/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [579/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [580/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [581/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [582/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [583/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [584/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [585/839], Loss: 0.0024\n",
            "Epoch [1/1], Batch [586/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [587/839], Loss: 0.0017\n",
            "Epoch [1/1], Batch [588/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [589/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [590/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [591/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [592/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [593/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [594/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [595/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [596/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [597/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [598/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [599/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [600/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [601/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [602/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [603/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [604/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [605/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [606/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [607/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [608/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [609/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [610/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [611/839], Loss: 0.0020\n",
            "Epoch [1/1], Batch [612/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [613/839], Loss: 0.0028\n",
            "Epoch [1/1], Batch [614/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [615/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [616/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [617/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [618/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [619/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [620/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [621/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [622/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [623/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [624/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [625/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [626/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [627/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [628/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [629/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [630/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [631/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [632/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [633/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [634/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [635/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [636/839], Loss: 0.0002\n",
            "Epoch [1/1], Batch [637/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [638/839], Loss: 0.0015\n",
            "Epoch [1/1], Batch [639/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [640/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [641/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [642/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [643/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [644/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [645/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [646/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [647/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [648/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [649/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [650/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [651/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [652/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [653/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [654/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [655/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [656/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [657/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [658/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [659/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [660/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [661/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [662/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [663/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [664/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [665/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [666/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [667/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [668/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [669/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [670/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [671/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [672/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [673/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [674/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [675/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [676/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [677/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [678/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [679/839], Loss: 0.0017\n",
            "Epoch [1/1], Batch [680/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [681/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [682/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [683/839], Loss: 0.0002\n",
            "Epoch [1/1], Batch [684/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [685/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [686/839], Loss: 0.0022\n",
            "Epoch [1/1], Batch [687/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [688/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [689/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [690/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [691/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [692/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [693/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [694/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [695/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [696/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [697/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [698/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [699/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [700/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [701/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [702/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [703/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [704/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [705/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [706/839], Loss: 0.0019\n",
            "Epoch [1/1], Batch [707/839], Loss: 0.0002\n",
            "Epoch [1/1], Batch [708/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [709/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [710/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [711/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [712/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [713/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [714/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [715/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [716/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [717/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [718/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [719/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [720/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [721/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [722/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [723/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [724/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [725/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [726/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [727/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [728/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [729/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [730/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [731/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [732/839], Loss: 0.0014\n",
            "Epoch [1/1], Batch [733/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [734/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [735/839], Loss: 0.0002\n",
            "Epoch [1/1], Batch [736/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [737/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [738/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [739/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [740/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [741/839], Loss: 0.0220\n",
            "Epoch [1/1], Batch [742/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [743/839], Loss: 0.0016\n",
            "Epoch [1/1], Batch [744/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [745/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [746/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [747/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [748/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [749/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [750/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [751/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [752/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [753/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [754/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [755/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [756/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [757/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [758/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [759/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [760/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [761/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [762/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [763/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [764/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [765/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [766/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [767/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [768/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [769/839], Loss: 0.0002\n",
            "Epoch [1/1], Batch [770/839], Loss: 0.0002\n",
            "Epoch [1/1], Batch [771/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [772/839], Loss: 0.0002\n",
            "Epoch [1/1], Batch [773/839], Loss: 0.0002\n",
            "Epoch [1/1], Batch [774/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [775/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [776/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [777/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [778/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [779/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [780/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [781/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [782/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [783/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [784/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [785/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [786/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [787/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [788/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [789/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [790/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [791/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [792/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [793/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [794/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [795/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [796/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [797/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [798/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [799/839], Loss: 0.0002\n",
            "Epoch [1/1], Batch [800/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [801/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [802/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [803/839], Loss: 0.0026\n",
            "Epoch [1/1], Batch [804/839], Loss: 0.0002\n",
            "Epoch [1/1], Batch [805/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [806/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [807/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [808/839], Loss: 0.0013\n",
            "Epoch [1/1], Batch [809/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [810/839], Loss: 0.0002\n",
            "Epoch [1/1], Batch [811/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [812/839], Loss: 0.0021\n",
            "Epoch [1/1], Batch [813/839], Loss: 0.0012\n",
            "Epoch [1/1], Batch [814/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [815/839], Loss: 0.0007\n",
            "Epoch [1/1], Batch [816/839], Loss: 0.0010\n",
            "Epoch [1/1], Batch [817/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [818/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [819/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [820/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [821/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [822/839], Loss: 0.0018\n",
            "Epoch [1/1], Batch [823/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [824/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [825/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [826/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [827/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [828/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [829/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [830/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [831/839], Loss: 0.0006\n",
            "Epoch [1/1], Batch [832/839], Loss: 0.0004\n",
            "Epoch [1/1], Batch [833/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [834/839], Loss: 0.0005\n",
            "Epoch [1/1], Batch [835/839], Loss: 0.0009\n",
            "Epoch [1/1], Batch [836/839], Loss: 0.0008\n",
            "Epoch [1/1], Batch [837/839], Loss: 0.0011\n",
            "Epoch [1/1], Batch [838/839], Loss: 0.0003\n",
            "Epoch [1/1], Batch [839/839], Loss: 0.0003\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    final_score = 0\n",
        "    model.train()\n",
        "    with tqdm.tqdm(train_loader) as pbar:\n",
        "        pbar.set_description(\"Epoch \" + str(epoch + 1))\n",
        "        for batch_idx, (inputs, labels) in enumerate(pbar):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            score = psnr(outputs, labels)\n",
        "            final_score+=score\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, PSNR: {final_score:.4f}\") # Loss: {loss.item():.4f},\n",
        "    torch.save(model.state_dict(), model_weight_path+'/weight_{}.pt'.format(epoch))\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQF0jq21ig0H"
      },
      "source": [
        "**Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm4h3NXmif-M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision.transforms import ToTensor, Resize, ToPILImage\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sys\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeZ6MHUziivB"
      },
      "outputs": [],
      "source": [
        "#모델 경로 입력\n",
        "model_weight_path='/content/weight/weight_0.pt'\n",
        "#LR 경로 입력\n",
        "input_file = '/content/Test_LR.zip'\n",
        "#SR 저장될 경로 입력\n",
        "output_zip_name = '/content/output.npy'\n",
        "\n",
        "input_folder=input_file[:-4]\n",
        "if not os.path.exists(input_folder):\n",
        "    os.system('unzip {} -d {}'.format(input_file,input_folder))\n",
        "output_folder=output_zip_name[:-4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSsMUI3BisHJ",
        "outputId": "654a0281-14b8-42e5-a566-93d9fbb569ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SRCNN(\n",
              "  (layer1): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
              "  (layer2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (layer3): Conv2d(32, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 모델 불러오기\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SRCNN().to(device)\n",
        "#터미널 창에 3번 째 입력으로 불러오고자 하는 weight 경로를 지정\n",
        "model.load_state_dict(torch.load(model_weight_path))\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOSvS4f-iszQ"
      },
      "outputs": [],
      "source": [
        "# Test 폴더의 이미지로 추론\n",
        "input_first_path=input_folder+'/'+os.listdir(input_folder)[0]\n",
        "input_first_img=cv2.imread(input_first_path)\n",
        "h,w=input_first_img.shape[0],input_first_img.shape[1]\n",
        "transform = Resize((int(h*2), int(w*2)), interpolation=Image.BICUBIC)\n",
        "to_image = ToPILImage()\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMie6GZaivsQ"
      },
      "outputs": [],
      "source": [
        "for filename in os.listdir(input_folder):\n",
        "    img = Image.open(os.path.join(input_folder, filename))\n",
        "    img_tensor = transform(img)\n",
        "    img_tensor = ToTensor()(img_tensor).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        upscaled = model(img_tensor)\n",
        "\n",
        "    result = to_image(upscaled[0].cpu())\n",
        "    result.save(os.path.join(output_folder, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2oZSbU7nSvX"
      },
      "source": [
        "**최종적으로 output.npy를 다운받아서 제춯**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "druw8Uvk0ruq",
        "outputId": "5c39af0f-9b04-497a-c717-2c9732d756dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "저장 완료 !! :  /content/output.npy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def images_to_npy(directory,filename):\n",
        "    # 폴더 내의 모든 파일을 가져옵니다.\n",
        "    filenames = [f for f in sorted(os.listdir(directory)) if os.path.isfile(os.path.join(directory, f))]\n",
        "\n",
        "    # 이미지 파일만 필터링합니다.\n",
        "    img_files = [f for f in filenames if f.lower().endswith(('.jpg'))]\n",
        "\n",
        "    # 모든 이미지를 로드하고 numpy 배열로 변환합니다.\n",
        "    images = []\n",
        "    for img_file in img_files:\n",
        "        path = os.path.join(directory, img_file)\n",
        "        img = Image.open(path)\n",
        "        img_array = np.array(img)\n",
        "        images.append(img_array)\n",
        "\n",
        "    # 이미지들을 하나의 numpy 배열로 변환합니다.\n",
        "    all_images = np.array(images)\n",
        "\n",
        "    # npy 파일로 저장합니다.\n",
        "    np.save(filename, all_images)\n",
        "\n",
        "# 함수 호출\n",
        "images_to_npy(output_folder,output_zip_name)\n",
        "print('저장 완료 !! : ', output_zip_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XisADK0RsCdW",
        "outputId": "b8929c14-ac00-4c1f-dee5-8e8b00f14cde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: typing_extensions 4.8.0\n",
            "Uninstalling typing_extensions-4.8.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/typing_extensions-4.8.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "Proceed (Y/n)? "
          ]
        }
      ],
      "source": [
        "!pip uninstall typing-extensions\n",
        "!pip install typing-extensions==4.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ltz8a0QqjqOc"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "from torchvision.transforms import ToTensor, Resize, ToPILImage\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sys\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1CGlWSjjtMn"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SRCNN().to(device)\n",
        "#터미널 창에 3번 째 입력으로 불러오고자 하는 weight 경로를 지정\n",
        "model.load_state_dict(torch.load('/content/weight/weight_0.pt'))\n",
        "model.eval()\n",
        "\n",
        "def image_mod(image):\n",
        "    to_image = ToPILImage()\n",
        "    h,w=image.size[1],image.size[0]\n",
        "    transform = Resize((int(h*2), int(w*2)), interpolation=Image.BICUBIC)\n",
        "\n",
        "    img_tensor = transform(image)\n",
        "    img_tensor = ToTensor()(img_tensor).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        upscaled = model(img_tensor)\n",
        "\n",
        "    result = to_image(upscaled[0].cpu())\n",
        "    return result\n",
        "\n",
        "\n",
        "demo = gr.Interface(\n",
        "    image_mod,\n",
        "    gr.Image(type=\"pil\"),\n",
        "    \"image\",\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
