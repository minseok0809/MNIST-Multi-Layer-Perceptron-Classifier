{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 한국어 지식 데이터의 관계 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[한국어 지식기반 관계 데이터](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=71633)\n",
        "<br>[한국어 지식 데이터의 관계 분류](https://aifactory.space/task/2658/leaderboard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from safetensors.torch import load_model, save_model\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ntrain_sentences= []\\nfor i in tqdm(train_data['sentence']):\\n    train_sentences.append(spacing(i))\\ntrain_data['sentence'] = train_sentences\\n\\ntest_sentences = []\\nfor i in tqdm(test_data['sentence']):\\n    test_sentences.append(spacing(i))\\ntest_data['sentence'] = test_sentences\\n\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = pd.read_csv('data/train_x.csv', encoding='utf-8')\n",
        "train_y = pd.read_csv('data/train_y.csv', encoding='utf-8')\n",
        "train_data['label'] = train_y['label']\n",
        "test_data = pd.read_csv('data/test_x.csv', encoding='utf-8')\n",
        "\n",
        "train_data['instruction'] = \"문장에서 \" + train_data['subj'] + \" 그리고 \" + train_data['obj'] + \"는 관계가 있다\"\n",
        "test_data['instruction'] = \"문장에서 \" + test_data['subj'] + \" 그리고 \" + test_data['obj'] + \"는 관계가 있다\"\n",
        "train_data['sentence'] = train_data['sentence'].str.replace('[^A-Za-z0-9ㄱ-ㅎ가-힣]', ' ', regex=True)\n",
        "test_data['sentence'] = test_data['sentence'].str.replace('[^A-Za-z0-9ㄱ-ㅎ가-힣]', ' ', regex=True)\n",
        "\n",
        "\"\"\"\n",
        "train_sentences= []\n",
        "for i in tqdm(train_data['sentence']):\n",
        "    train_sentences.append(spacing(i))\n",
        "train_data['sentence'] = train_sentences\n",
        "\n",
        "test_sentences = []\n",
        "for i in tqdm(test_data['sentence']):\n",
        "    test_sentences.append(spacing(i))\n",
        "test_data['sentence'] = test_sentences\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max Sentence Length:  497\n",
            "Min Sentence Length:  13\n",
            "Mean Sentence Lengh:  84.307\n",
            "Max Instruction Length:  62\n",
            "Min Instruction Length:  21\n",
            "Mean Instruction Lengh:  29.139\n",
            "The Number of Label:  19\n"
          ]
        }
      ],
      "source": [
        "sentence_max_len = np.max(train_data['sentence'].str.len())\n",
        "sentence_min_len = np.min(train_data['sentence'].str.len())\n",
        "sentence_mean_len = np.mean(train_data['sentence'].str.len())\n",
        "instruction_max_len = np.max(train_data['instruction'].str.len())\n",
        "instruction_min_len = np.min(train_data['instruction'].str.len())\n",
        "instruction_mean_len = np.mean(train_data['instruction'].str.len())\n",
        "label_num = len(set(train_data['label']))\n",
        "\n",
        "print('Max Sentence Length: ', sentence_max_len)\n",
        "print('Min Sentence Length: ', sentence_min_len)\n",
        "print('Mean Sentence Lengh: ', sentence_mean_len)\n",
        "print('Max Instruction Length: ', instruction_max_len)\n",
        "print('Min Instruction Length: ', instruction_min_len)\n",
        "print('Mean Instruction Lengh: ', instruction_mean_len)\n",
        "print('The Number of Label: ', label_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at kykim/bert-kor-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'kykim/bert-kor-base' # 'kykim/bert-kor-base'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "config.num_labels = 20\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset, eval_dataset = train_test_split(train_data, test_size=0.1, shuffle=True, stratify=train_data['label'])\n",
        "\n",
        "tokenized_train = tokenizer(\n",
        "    list(train_dataset['sentence']),\n",
        "    list(train_dataset['instruction']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=512, \n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "tokenized_eval = tokenizer(\n",
        "    list(eval_dataset['sentence']),\n",
        "    list(eval_dataset['instruction']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=512,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "# print(tokenized_train['input_ids'][0])\n",
        "# print(tokenizer.decode(tokenized_train['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, pair_dataset, label):\n",
        "        self.pair_dataset = pair_dataset\n",
        "        self.label = label\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
        "        item['label'] = torch.tensor(self.label[idx])\n",
        "        \n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = BERTDataset(tokenized_train, train_dataset['label'].values)\n",
        "eval_dataset = BERTDataset(tokenized_eval, eval_dataset['label'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "  \n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  probs = pred.predictions\n",
        "\n",
        "  acc = accuracy_score(labels, preds) \n",
        "\n",
        "  return {\n",
        "      'accuracy': acc,\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_ars = TrainingArguments(\n",
        "    output_dir='output',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    save_total_limit=10,\n",
        "    save_strategy='epoch', \n",
        "    evaluation_strategy='epoch',\n",
        "    load_best_model_at_end = True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_ars,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics, \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 01:17, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.314619</td>\n",
              "      <td>0.260000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.146322</td>\n",
              "      <td>0.260000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.993048</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.858849</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.769520</td>\n",
              "      <td>0.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.756153</td>\n",
              "      <td>0.470000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.826705</td>\n",
              "      <td>0.430000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.848759</td>\n",
              "      <td>0.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.872167</td>\n",
              "      <td>0.490000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.896422</td>\n",
              "      <td>0.490000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = 'output' # 'kykim/bert-kor-base'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_safetensors=True)\n",
        "\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "config.num_labels = 20\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_test = tokenizer(\n",
        "    list(test_data['sentence']),\n",
        "    list(test_data['instruction']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=512, \n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataset = BERTDataset(tokenized_test, train_dataset['label'].values[0:len(test_data['sentence'])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_args = TrainingArguments(\n",
        "    output_dir = 'test_output',\n",
        "    do_train = False,\n",
        "    do_predict = True,\n",
        "    per_device_eval_batch_size = 32,   \n",
        "    dataloader_drop_last = False    \n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "              model = model, \n",
        "              args = test_args, \n",
        "              compute_metrics = compute_metrics)\n",
        "\n",
        "predictions, labels, metrics = trainer.predict(test_dataset)\n",
        "predictions = np.argmax(predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_csv = pd.DataFrame(labels)\n",
        "result_csv.columns = {\"label\"}\n",
        "\n",
        "result_csv.to_csv(\"result.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[월간 데이콘 한국어 문장 관계 분류 경진대회 - Hugging Face를 활용한 Modeling(public: 0.841)](https://github.com/ldj7672/Deep-Learning-Tutorials/blob/main/HuggingFace/HuggingFace_SwinT_image_classification.ipynb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
