{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [AI7008] AI실전연구프로젝트2\n",
    "Professor: 김태영\n",
    "<br><br>Student: Minseok Yang (msyang0809@khu.ac.kr)\n",
    "<br>Number: 2023310384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Project] 낚시성 기사 탐지 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "<br/>Random Baseline \n",
    "<br/>Model Training\n",
    "<br/>Model Inference\n",
    "<br/>Clickbait Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets\n",
    "%jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import make_train_valid_json_xlsx_file_path_list\n",
    "from data_preprocessing import write_jsontext_to_xlsx_file_with_batch_size\n",
    "from data_preprocessing import write_xlsxtext_to_list_merge_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_list = ['data/aihub_clickbait/Training/Labeling'+ '/**/*.json', \n",
    "                  'data/aihub_clickbait/Validation/Labeling'+ '/**/*.json']\n",
    "xlsx_path_list = ['data/aihub_clickbait/df/clickbait_classfication_train_', \n",
    "                 'data/aihub_clickbait/df/clickbait_classfication_valid_',\n",
    "                 'data/aihub_clickbait/df/nonclickbait_classfication_train_', \n",
    "                 'data/aihub_clickbait/df/nonclickbait_classfication_valid_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train clickbait json file: 296035\n",
      "The number of valid clickbait json file: 36991\n",
      "The number of clickbait json file: 333026\n",
      "\n",
      "The number of train nonclickbait json file: 290689\n",
      "The number of valid nonclickbait json file: 36330\n",
      "The number of nonclickbait json file: 327019\n",
      "\n",
      "The number of train json file: 586724\n",
      "The number of valid json file: 73321\n",
      "The number of json file: 660045\n"
     ]
    }
   ],
   "source": [
    "train_clickbait_json_file_path_list, train_nonclickbait_json_file_path_list, \\\n",
    "valid_clickbait_json_file_path_list, valid_nonclickbait_json_file_path_list, \\\n",
    "train_clickbait_xlsx_file_path_list, train_nonclickbait_xlsx_file_path_list, \\\n",
    "valid_clickbait_xlsx_file_path_list, valid_nonclickbait_xlsx_file_path_list = \\\n",
    "make_train_valid_json_xlsx_file_path_list(json_path_list, xlsx_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Size]\n",
      "The number of xlsx file: 296\n",
      "\n",
      "[Order]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "297it [54:54, 11.09s/it]                         \n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_xlsx_file_with_batch_size(train_clickbait_json_file_path_list, train_clickbait_xlsx_file_path_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Size]\n",
      "The number of xlsx file: 290\n",
      "\n",
      "[Order]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "291it [55:14, 11.39s/it]                         \n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_xlsx_file_with_batch_size(train_nonclickbait_json_file_path_list, train_nonclickbait_xlsx_file_path_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Size]\n",
      "The number of xlsx file: 36\n",
      "\n",
      "[Order]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [06:46, 11.00s/it]                        \n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_xlsx_file_with_batch_size(valid_clickbait_json_file_path_list, valid_clickbait_xlsx_file_path_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Size]\n",
      "The number of xlsx file: 36\n",
      "\n",
      "[Order]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [07:19, 11.87s/it]                        \n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "write_jsontext_to_xlsx_file_with_batch_size(valid_nonclickbait_json_file_path_list, valid_nonclickbait_xlsx_file_path_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train text: 580856\n",
      "The number of valid text: 73321\n",
      "The number of test text: 5868\n"
     ]
    }
   ],
   "source": [
    "xlsx_folder = './data/aihub_clickbait_detection/df/*.xlsx'\n",
    "dataset_folder = './data/aihub_clickbait_detection/dataset/'\n",
    "train_list, valid_list, test_list = write_xlsxtext_to_list_merge_file(xlsx_folder, dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.51 %\n"
     ]
    }
   ],
   "source": [
    "!python -m score \\\n",
    "    --dataset_folder './data/aihub_clickbait_detection/dataset/' \\\n",
    "    --label [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
    "%pip install protobuf\n",
    "%pip install --upgrade protobuf==3.20.1\n",
    "%pip install openpyxl\n",
    "%pip install --upgrade numpy==1.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from tokenization_kobert import KoBERTTokenizer\n",
    "\n",
    "save_path = \"./kobert\"\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc, os\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1:   0%|                                         | 0/9076 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████████████████████████| 9076/9076 [52:15<00:00,  2.89it/s]\n",
      "Train Time 00:52:15    Train Loss: 4867.8861    Train Accuracy: 0.7212\n",
      "Epoch 2: 100%|██████████████████████████████| 9076/9076 [53:58<00:00,  2.80it/s]\n",
      "Train Time 00:53:58    Train Loss: 4100.3653    Train Accuracy: 0.7867\n",
      "Epoch 3: 100%|██████████████████████████████| 9076/9076 [53:58<00:00,  2.80it/s]\n",
      "Train Time 00:53:58    Train Loss: 3610.4118    Train Accuracy: 0.8203\n",
      "Epoch 4: 100%|██████████████████████████████| 9076/9076 [54:03<00:00,  2.80it/s]\n",
      "Train Time 00:54:03    Train Loss: 3136.4276    Train Accuracy: 0.85\n",
      "Epoch 5: 100%|██████████████████████████████| 9076/9076 [54:01<00:00,  2.80it/s]\n",
      "Train Time 00:54:01    Train Loss: 2674.4274    Train Accuracy: 0.8766\n",
      "Epoch 6: 100%|██████████████████████████████| 9076/9076 [54:00<00:00,  2.80it/s]\n",
      "Train Time 00:54:00    Train Loss: 2243.5024    Train Accuracy: 0.9005\n",
      "Epoch 7: 100%|██████████████████████████████| 9076/9076 [53:58<00:00,  2.80it/s]\n",
      "Train Time 00:53:58    Train Loss: 1874.5743    Train Accuracy: 0.9193\n",
      "Epoch 8: 100%|██████████████████████████████| 9076/9076 [54:20<00:00,  2.78it/s]\n",
      "Train Time 00:54:20    Train Loss: 1550.9449    Train Accuracy: 0.9347\n",
      "Epoch 9: 100%|██████████████████████████████| 9076/9076 [54:18<00:00,  2.79it/s]\n",
      "Train Time 00:54:18    Train Loss: 1288.9194    Train Accuracy: 0.9468\n",
      "Epoch 10: 100%|█████████████████████████████| 9076/9076 [54:23<00:00,  2.78it/s]\n",
      "Train Time 00:54:23    Train Loss: 1082.3514    Train Accuracy: 0.9559\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████████████████████████| 1146/1146 [03:32<00:00,  5.40it/s]\n",
      "Valid Time 00:03:32    Valid Loss: 1005.4296    Valid Accuracy: 0.5672\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 2: 100%|██████████████████████████████| 1146/1146 [03:35<00:00,  5.32it/s]\n",
      "Valid Time 00:03:35    Valid Loss: 1190.8559    Valid Accuracy: 0.5733\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 3: 100%|██████████████████████████████| 1146/1146 [03:36<00:00,  5.29it/s]\n",
      "Valid Time 00:03:36    Valid Loss: 1282.4672    Valid Accuracy: 0.5851\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 4: 100%|██████████████████████████████| 1146/1146 [03:36<00:00,  5.29it/s]\n",
      "Valid Time 00:03:36    Valid Loss: 1434.1065    Valid Accuracy: 0.5756\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 5: 100%|██████████████████████████████| 1146/1146 [03:35<00:00,  5.31it/s]\n",
      "Valid Time 00:03:35    Valid Loss: 1520.2188    Valid Accuracy: 0.5798\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 6: 100%|██████████████████████████████| 1146/1146 [03:35<00:00,  5.31it/s]\n",
      "Valid Time 00:03:35    Valid Loss: 1672.7669    Valid Accuracy: 0.583\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 7: 100%|██████████████████████████████| 1146/1146 [03:35<00:00,  5.31it/s]\n",
      "Valid Time 00:03:35    Valid Loss: 1888.6293    Valid Accuracy: 0.5769\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 8: 100%|██████████████████████████████| 1146/1146 [03:35<00:00,  5.32it/s]\n",
      "Valid Time 00:03:35    Valid Loss: 2075.0215    Valid Accuracy: 0.5739\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 9: 100%|██████████████████████████████| 1146/1146 [03:34<00:00,  5.33it/s]\n",
      "Valid Time 00:03:34    Valid Loss: 2185.2659    Valid Accuracy: 0.5731\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 10: 100%|█████████████████████████████| 1146/1146 [03:35<00:00,  5.32it/s]\n",
      "Valid Time 00:03:35    Valid Loss: 2362.4691    Valid Accuracy: 0.5783\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "!python -m train \\\n",
    "    --model_name 'klue/roberta-base' \\\n",
    "    --data_path './data/aihub_clickbait_detection/dataset/' \\\n",
    "    --train_dataset 'train_dataset.xlsx'  \\\n",
    "    --valid_dataset 'valid_dataset.xlsx' \\\n",
    "    --save_model_path './model/roberta_base/'  \\\n",
    "    --log_path './log/roberta_base/clickabait_detection_training_log.xlsx'  \\\n",
    "    --epoch 10  \\\n",
    "    --batch_size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1:   0%|                                         | 0/9076 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|████████████████████████████| 9076/9076 [1:07:39<00:00,  2.24it/s]\n",
      "Train Time 01:07:39    Train Loss: 5261.1873    Train Accuracy: 0.6784\n",
      "Epoch 2: 100%|████████████████████████████| 9076/9076 [1:11:06<00:00,  2.13it/s]\n",
      "Train Time 01:11:06    Train Loss: 4573.9952    Train Accuracy: 0.7473\n",
      "Epoch 3: 100%|████████████████████████████| 9076/9076 [1:11:04<00:00,  2.13it/s]\n",
      "Train Time 01:11:04    Train Loss: 4126.7503    Train Accuracy: 0.7823\n",
      "Epoch 4: 100%|████████████████████████████| 9076/9076 [1:11:06<00:00,  2.13it/s]\n",
      "Train Time 01:11:06    Train Loss: 3690.7799    Train Accuracy: 0.8136\n",
      "Epoch 5: 100%|████████████████████████████| 9076/9076 [1:11:07<00:00,  2.13it/s]\n",
      "Train Time 01:11:07    Train Loss: 3251.1418    Train Accuracy: 0.8422\n",
      "Epoch 6: 100%|████████████████████████████| 9076/9076 [1:11:04<00:00,  2.13it/s]\n",
      "Train Time 01:11:04    Train Loss: 2833.3891    Train Accuracy: 0.8678\n",
      "Epoch 7: 100%|████████████████████████████| 9076/9076 [1:11:02<00:00,  2.13it/s]\n",
      "Train Time 01:11:02    Train Loss: 2433.9315    Train Accuracy: 0.8899\n",
      "Epoch 8: 100%|████████████████████████████| 9076/9076 [1:11:35<00:00,  2.11it/s]\n",
      "Train Time 01:11:35    Train Loss: 2083.0508    Train Accuracy: 0.9086\n",
      "Epoch 9: 100%|████████████████████████████| 9076/9076 [1:11:32<00:00,  2.11it/s]\n",
      "Train Time 01:11:32    Train Loss: 1777.1254    Train Accuracy: 0.9233\n",
      "Epoch 10: 100%|███████████████████████████| 9076/9076 [1:11:30<00:00,  2.12it/s]\n",
      "Train Time 01:11:30    Train Loss: 1523.1468    Train Accuracy: 0.9355\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████████████████████████| 1146/1146 [05:46<00:00,  3.31it/s]\n",
      "Valid Time 00:05:46    Valid Loss: 914.7448    Valid Accuracy: 0.561\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 2: 100%|██████████████████████████████| 1146/1146 [05:47<00:00,  3.29it/s]\n",
      "Valid Time 00:05:47    Valid Loss: 997.1389    Valid Accuracy: 0.5671\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 3: 100%|██████████████████████████████| 1146/1146 [05:51<00:00,  3.26it/s]\n",
      "Valid Time 00:05:51    Valid Loss: 1113.5824    Valid Accuracy: 0.5691\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 4: 100%|██████████████████████████████| 1146/1146 [05:50<00:00,  3.27it/s]\n",
      "Valid Time 00:05:50    Valid Loss: 1203.7708    Valid Accuracy: 0.5709\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 5: 100%|██████████████████████████████| 1146/1146 [05:50<00:00,  3.27it/s]\n",
      "Valid Time 00:05:50    Valid Loss: 1324.0847    Valid Accuracy: 0.5624\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 6: 100%|██████████████████████████████| 1146/1146 [05:49<00:00,  3.27it/s]\n",
      "Valid Time 00:05:49    Valid Loss: 1483.5554    Valid Accuracy: 0.5651\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 7: 100%|██████████████████████████████| 1146/1146 [05:49<00:00,  3.28it/s]\n",
      "Valid Time 00:05:49    Valid Loss: 1574.7254    Valid Accuracy: 0.5632\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 8: 100%|██████████████████████████████| 1146/1146 [05:50<00:00,  3.27it/s]\n",
      "Valid Time 00:05:50    Valid Loss: 1844.3202    Valid Accuracy: 0.5615\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 9: 100%|██████████████████████████████| 1146/1146 [05:50<00:00,  3.27it/s]\n",
      "Valid Time 00:05:50    Valid Loss: 1996.124    Valid Accuracy: 0.5643\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 10: 100%|█████████████████████████████| 1146/1146 [05:49<00:00,  3.28it/s]\n",
      "Valid Time 00:05:49    Valid Loss: 2172.5182    Valid Accuracy: 0.5611\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "!python -m train \\\n",
    "    --model_name 'klue/roberta-base' \\\n",
    "    --data_path './data/aihub_clickbait_detection/dataset_preprocessing/' \\\n",
    "    --train_dataset 'train_dataset.xlsx'  \\\n",
    "    --valid_dataset 'valid_dataset.xlsx' \\\n",
    "    --save_model_path './model/roberta_base_preprocessing/'  \\\n",
    "    --log_path './log/roberta_base_preprocessing/clickabait_detection_training_log.xlsx'  \\\n",
    "    --epoch 10  \\\n",
    "    --batch_size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1:   0%|                                        | 0/36304 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████████████████████| 36304/36304 [2:11:09<00:00,  4.61it/s]\n",
      "Train Time 02:11:09    Train Loss: 18964.6072    Train Accuracy: 0.7353\n",
      "Epoch 2: 100%|██████████████████████████| 36304/36304 [2:13:11<00:00,  4.54it/s]\n",
      "Train Time 02:13:11    Train Loss: 15665.4326    Train Accuracy: 0.8015\n",
      "Epoch 3: 100%|██████████████████████████| 36304/36304 [2:13:02<00:00,  4.55it/s]\n",
      "Train Time 02:13:02    Train Loss: 12936.0197    Train Accuracy: 0.8457\n",
      "Epoch 4: 100%|██████████████████████████| 36304/36304 [2:13:06<00:00,  4.55it/s]\n",
      "Train Time 02:13:06    Train Loss: 10105.2154    Train Accuracy: 0.8874\n",
      "Epoch 5: 100%|██████████████████████████| 36304/36304 [2:13:01<00:00,  4.55it/s]\n",
      "Train Time 02:13:01    Train Loss: 7571.3085    Train Accuracy: 0.9197\n",
      "Epoch 6: 100%|██████████████████████████| 36304/36304 [2:12:59<00:00,  4.55it/s]\n",
      "Train Time 02:12:59    Train Loss: 5712.9334    Train Accuracy: 0.9415\n",
      "Epoch 7: 100%|██████████████████████████| 36304/36304 [2:12:49<00:00,  4.56it/s]\n",
      "Train Time 02:12:49    Train Loss: 4468.8538    Train Accuracy: 0.955\n",
      "Epoch 8: 100%|██████████████████████████| 36304/36304 [2:12:54<00:00,  4.55it/s]\n",
      "Train Time 02:12:54    Train Loss: 3640.8082    Train Accuracy: 0.9634\n",
      "Epoch 9: 100%|██████████████████████████| 36304/36304 [2:12:59<00:00,  4.55it/s]\n",
      "Train Time 02:12:59    Train Loss: 3105.2071    Train Accuracy: 0.9693\n",
      "Epoch 10: 100%|█████████████████████████| 36304/36304 [2:13:01<00:00,  4.55it/s]\n",
      "Train Time 02:13:01    Train Loss: 2704.3033    Train Accuracy: 0.973\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████████████████████████| 4583/4583 [06:28<00:00, 11.80it/s]\n",
      "Valid Time 00:06:28    Valid Loss: 4152.4873    Valid Accuracy: 0.5809\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 2: 100%|██████████████████████████████| 4583/4583 [06:31<00:00, 11.71it/s]\n",
      "Valid Time 00:06:31    Valid Loss: 4397.1312    Valid Accuracy: 0.5801\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 3: 100%|██████████████████████████████| 4583/4583 [06:31<00:00, 11.69it/s]\n",
      "Valid Time 00:06:31    Valid Loss: 5498.8794    Valid Accuracy: 0.5789\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 4: 100%|██████████████████████████████| 4583/4583 [06:33<00:00, 11.66it/s]\n",
      "Valid Time 00:06:33    Valid Loss: 6354.2207    Valid Accuracy: 0.5824\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 5: 100%|██████████████████████████████| 4583/4583 [06:32<00:00, 11.68it/s]\n",
      "Valid Time 00:06:32    Valid Loss: 6995.3785    Valid Accuracy: 0.5758\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 6: 100%|██████████████████████████████| 4583/4583 [06:34<00:00, 11.62it/s]\n",
      "Valid Time 00:06:34    Valid Loss: 7793.4677    Valid Accuracy: 0.5801\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 7: 100%|██████████████████████████████| 4583/4583 [06:32<00:00, 11.69it/s]\n",
      "Valid Time 00:06:32    Valid Loss: 8504.0151    Valid Accuracy: 0.5784\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 8: 100%|██████████████████████████████| 4583/4583 [06:33<00:00, 11.66it/s]\n",
      "Valid Time 00:06:33    Valid Loss: 9436.6194    Valid Accuracy: 0.5754\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 9: 100%|██████████████████████████████| 4583/4583 [06:33<00:00, 11.65it/s]\n",
      "Valid Time 00:06:33    Valid Loss: 9705.5826    Valid Accuracy: 0.5778\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 10: 100%|█████████████████████████████| 4583/4583 [06:32<00:00, 11.69it/s]\n",
      "Valid Time 00:06:32    Valid Loss: 10549.8893    Valid Accuracy: 0.5758\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "!python -m train \\\n",
    "    --model_name 'klue/roberta-large' \\\n",
    "    --data_path './data/aihub_clickbait_detection/dataset/' \\\n",
    "    --train_dataset 'train_dataset.xlsx'  \\\n",
    "    --valid_dataset 'valid_dataset.xlsx' \\\n",
    "    --save_model_path './model/roberta_large/'  \\\n",
    "    --log_path './log/roberta_large/clickabait_detection_training_log.xlsx'  \\\n",
    "    --epoch 10  \\\n",
    "    --batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1:   0%|                                        | 0/36304 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████████████████████| 36304/36304 [2:27:22<00:00,  4.11it/s]\n",
      "Train Time 02:27:22    Train Loss: 20552.6475    Train Accuracy: 0.6938\n",
      "Epoch 2: 100%|██████████████████████████| 36304/36304 [2:30:27<00:00,  4.02it/s]\n",
      "Train Time 02:30:27    Train Loss: 17775.135    Train Accuracy: 0.7592\n",
      "Epoch 3: 100%|██████████████████████████| 36304/36304 [2:30:28<00:00,  4.02it/s]\n",
      "Train Time 02:30:28    Train Loss: 15545.79    Train Accuracy: 0.8015\n",
      "Epoch 4: 100%|██████████████████████████| 36304/36304 [2:30:27<00:00,  4.02it/s]\n",
      "Train Time 02:30:27    Train Loss: 12959.54    Train Accuracy: 0.8448\n",
      "Epoch 5: 100%|██████████████████████████| 36304/36304 [2:30:19<00:00,  4.02it/s]\n",
      "Train Time 02:30:19    Train Loss: 10405.1538    Train Accuracy: 0.8823\n",
      "Epoch 6: 100%|██████████████████████████| 36304/36304 [2:30:25<00:00,  4.02it/s]\n",
      "Train Time 02:30:25    Train Loss: 8150.3324    Train Accuracy: 0.912\n",
      "Epoch 7: 100%|██████████████████████████| 36304/36304 [2:29:59<00:00,  4.03it/s]\n",
      "Train Time 02:29:59    Train Loss: 6426.5138    Train Accuracy: 0.9328\n",
      "Epoch 8: 100%|██████████████████████████| 36304/36304 [2:29:45<00:00,  4.04it/s]\n",
      "Train Time 02:29:45    Train Loss: 5225.709    Train Accuracy: 0.9469\n",
      "Epoch 9: 100%|██████████████████████████| 36304/36304 [2:29:48<00:00,  4.04it/s]\n",
      "Train Time 02:29:48    Train Loss: 4400.7064    Train Accuracy: 0.9556\n",
      "Epoch 10: 100%|█████████████████████████| 36304/36304 [2:29:46<00:00,  4.04it/s]\n",
      "Train Time 02:29:46    Train Loss: 3771.9415    Train Accuracy: 0.9622\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████████████████████████| 4583/4583 [08:43<00:00,  8.75it/s]\n",
      "Valid Time 00:08:43    Valid Loss: 3881.1598    Valid Accuracy: 0.5611\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 2: 100%|██████████████████████████████| 4583/4583 [08:48<00:00,  8.68it/s]\n",
      "Valid Time 00:08:48    Valid Loss: 4080.0037    Valid Accuracy: 0.5801\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 3: 100%|██████████████████████████████| 4583/4583 [08:49<00:00,  8.66it/s]\n",
      "Valid Time 00:08:49    Valid Loss: 4880.5204    Valid Accuracy: 0.569\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 4: 100%|██████████████████████████████| 4583/4583 [08:50<00:00,  8.64it/s]\n",
      "Valid Time 00:08:50    Valid Loss: 5429.189    Valid Accuracy: 0.5716\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 5: 100%|██████████████████████████████| 4583/4583 [08:49<00:00,  8.66it/s]\n",
      "Valid Time 00:08:49    Valid Loss: 6061.9463    Valid Accuracy: 0.5661\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 6: 100%|██████████████████████████████| 4583/4583 [08:48<00:00,  8.67it/s]\n",
      "Valid Time 00:08:48    Valid Loss: 6454.7638    Valid Accuracy: 0.5733\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 7: 100%|██████████████████████████████| 4583/4583 [08:50<00:00,  8.64it/s]\n",
      "Valid Time 00:08:50    Valid Loss: 7637.6457    Valid Accuracy: 0.5657\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 8: 100%|██████████████████████████████| 4583/4583 [08:49<00:00,  8.66it/s]\n",
      "Valid Time 00:08:49    Valid Loss: 7985.3564    Valid Accuracy: 0.5645\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 9: 100%|██████████████████████████████| 4583/4583 [08:48<00:00,  8.68it/s]\n",
      "Valid Time 00:08:48    Valid Loss: 8505.4458    Valid Accuracy: 0.5678\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 10: 100%|█████████████████████████████| 4583/4583 [08:49<00:00,  8.66it/s]\n",
      "Valid Time 00:08:49    Valid Loss: 8325.492    Valid Accuracy: 0.5663\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "!python -m train \\\n",
    "    --model_name 'klue/roberta-large' \\\n",
    "    --data_path './data/aihub_clickbait_detection/dataset_preprocessing/' \\\n",
    "    --train_dataset 'train_dataset.xlsx'  \\\n",
    "    --valid_dataset 'valid_dataset.xlsx' \\\n",
    "    --save_model_path './model/roberta_large_preprocessing/'  \\\n",
    "    --log_path './log/roberta_large_preprocessing/clickabait_detection_training_log.xlsx'  \\\n",
    "    --epoch 10  \\\n",
    "    --batch_size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                  | 0/5868 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████| 5868/5868 [00:38<00:00, 151.57it/s]\n",
      "Test Time 00:00:38    Test Accuracy: 0.8546\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "!python -m inference \\\n",
    "    --model_name 'klue/roberta-base' \\\n",
    "    --data_path './data/aihub_clickbait_detection/dataset/' \\\n",
    "    --test_dataset 'test_dataset.xlsx'  \\\n",
    "    --load_model_path './model/roberta_base/'  \\\n",
    "    --log_path './log/roberta_base/clickabait_detection_evaluation_log.xlsx' \\\n",
    "    --inference_log_path './log/roberta_base/clickabait_detection_inference_log.xlsx' \\\n",
    "    --epoch 10  \\\n",
    "    --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                  | 0/5868 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████| 5868/5868 [00:47<00:00, 123.21it/s]\n",
      "Test Time 00:00:47    Test Accuracy: 0.8386\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "!python -m inference \\\n",
    "    --model_name 'klue/roberta-base' \\\n",
    "    --data_path './data/aihub_clickbait_detection/dataset_preprocessing/' \\\n",
    "    --test_dataset 'test_dataset.xlsx'  \\\n",
    "    --load_model_path './model/roberta_base_preprocessing/'  \\\n",
    "    --log_path './log/roberta_base/clickabait_detection_evaluation_log.xlsx' \\\n",
    "    --inference_log_path './log/roberta_base_preprocessing/clickabait_detection_inference_log.xlsx' \\\n",
    "    --epoch 10  \\\n",
    "    --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                  | 0/5868 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████| 5868/5868 [01:06<00:00, 88.59it/s]\n",
      "Test Time 00:01:06    Test Accuracy: 0.8671\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "!python -m inference \\\n",
    "    --model_name 'klue/roberta-large' \\\n",
    "    --data_path './data/aihub_clickbait_detection/dataset/' \\\n",
    "    --test_dataset 'test_dataset.xlsx'  \\\n",
    "    --load_model_path './model/roberta_large/'  \\\n",
    "    --log_path './log/roberta_large/clickabait_detection_evaluation_log.xlsx' \\\n",
    "    --inference_log_path './log/roberta_large/clickabait_detection_inference_log.xlsx' \\\n",
    "    --epoch 10  \\\n",
    "    --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                  | 0/5868 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████| 5868/5868 [01:16<00:00, 76.91it/s]\n",
      "Test Time 00:01:16    Test Accuracy: 0.842\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "!python -m inference \\\n",
    "    --model_name 'klue/roberta-large' \\\n",
    "    --data_path './data/aihub_clickbait_detection/dataset_preprocessing/' \\\n",
    "    --test_dataset 'test_dataset.xlsx'  \\\n",
    "    --load_model_path './model/roberta_large_preprocessing/'  \\\n",
    "    --log_path './log/roberta_large/clickabait_detection_evaluation_log.xlsx' \\\n",
    "    --inference_log_path './log/roberta_large_preprocessing/clickabait_detection_inference_log.xlsx' \\\n",
    "    --epoch 10  \\\n",
    "    --batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clickbait Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.98it/s]\n",
      "Time 00:00:00    Accuracy: 0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "!python -m clickbait_detection \\\n",
    "    --model_name 'klue/roberta-base' \\\n",
    "    --news_article_text_folder './data/aihub_clickbait_detection/news_article/' \\\n",
    "    --data_path './data/aihub_clickbait_detection/inference/' \\\n",
    "    --dataset 'dataset.xlsx'  \\\n",
    "    --label 1  \\\n",
    "    --load_model_path './model/roberta_base/'  \\\n",
    "    --clickbait_detection_log_path './log/roberta_base/clickabait_detection_log.xlsx' \\\n",
    "    --epoch 10  \\\n",
    "    --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "Time 00:00:00    Accuracy: 1\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "!python -m clickbait_detection \\\n",
    "    --model_name 'klue/roberta-base' \\\n",
    "    --news_article_text_folder './data/aihub_clickbait_detection/news_article/' \\\n",
    "    --data_path './data/aihub_clickbait_detection/inference/' \\\n",
    "    --dataset 'dataset.xlsx'  \\\n",
    "    --label 1  \\\n",
    "    --load_model_path './model/roberta_base_preprocessing/'  \\\n",
    "    --clickbait_detection_log_path './log/roberta_base_preprocessing/clickabait_detection_log.xlsx' \\\n",
    "    --epoch 10  \\\n",
    "    --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "Time 00:00:00    Accuracy: 1\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "!python -m clickbait_detection \\\n",
    "    --model_name 'klue/roberta-large' \\\n",
    "    --news_article_text_folder './data/aihub_clickbait_detection/news_article/' \\\n",
    "    --data_path './data/aihub_clickbait_detection/inference/' \\\n",
    "    --dataset 'dataset.xlsx'  \\\n",
    "    --label 1  \\\n",
    "    --load_model_path './model/roberta_large/'  \\\n",
    "    --clickbait_detection_log_path './log/roberta_large/clickabait_detection_log.xlsx' \\\n",
    "    --epoch 10  \\\n",
    "    --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "Time 00:00:00    Accuracy: 0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "!python -m clickbait_detection \\\n",
    "    --model_name 'klue/roberta-large' \\\n",
    "    --news_article_text_folder './data/aihub_clickbait_detection/news_article/' \\\n",
    "    --data_path './data/aihub_clickbait_detection/inference/' \\\n",
    "    --dataset 'dataset.xlsx'  \\\n",
    "    --label 1  \\\n",
    "    --load_model_path './model/roberta_large_preprocessing/'  \\\n",
    "    --clickbait_detection_log_path './log/roberta_large_preprocessing/clickabait_detection_log.xlsx' \\\n",
    "    --epoch 10  \\\n",
    "    --batch_size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data</b>\n",
    "<br>[AI Hub 낚시성 기사 탐지 데이터](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=71338)\n",
    "\n",
    "<br><b>Paper</b>\n",
    "<br>[Yinhan Liu et al. Robustly Optimized BERT Pretraining Approach. 2019.](https://aclanthology.org/N16-1174/)\n",
    "<br>[Kevin Clark et al. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. ICLR, 2020](https://arxiv.org/abs/2003.10555)\n",
    "<br>[Zichao Yang et al. Hierarchical Attention Networks for Document Classification. NAACL, 2016.](https://aclanthology.org/N16-1174/)\n",
    "<br>[Yongjie Wang et al. On the Use of Bert for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation. NAACL, 2022.](https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11485497)\n",
    "<br>[한상우, 온병원. 다중 계층 BERT를 활용한 낚시성 기사 탐지 모델. 한국정보기술학회, 2023.](https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11485497)\n",
    "\n",
    "<br><b>HuggingFace</b>\n",
    "<br>[klue/roberta-base](https://huggingface.co/klue/roberta-base)\n",
    "<br>[klue/roberta-large](https://huggingface.co/klue/roberta-large)\n",
    "\n",
    "<br><b>Github</b>\n",
    "<br>[hhk1364/Fishing-Article-Detection-Project-Backend](https://github.com/hhk1364/Fishing-Article-Detection-Project-Backend)\n",
    "<br>[SKTBrain/KoBERT](https://github.com/SKTBrain/KoBERT)\n",
    "<br>[monologg/KoBERT-Transformers](https://github.com/monologg/KoBERT-Transformers)\n",
    "<br>[monologg/KoELECTRA](https://github.com/monologg/KoELECTRA)\n",
    "<br>[jaehyeongAN/KoELECTRA-finetuned-sentiment-analysis](https://github.com/jaehyeongAN/KoELECTRA-finetuned-sentiment-analysis)\n",
    "<br>[lkkaram/korean-frown-sentence-classifier](https://github.com/lkkaram/korean-frown-sentence-classifier)\n",
    "<br>[kimwoonggon/publicservant_AI](https://github.com/kimwoonggon/publicservant_AI/)\n",
    "<br>[lingochamp/Multi-Scale-BERT-AES](https://github.com/lingochamp/Multi-Scale-BERT-AES/)\n",
    "<br>[Doheon/NewsClassification-KoBERT](https://github.com/Doheon/NewsClassification-KoBERT)\n",
    "<br>[jaylnne/nsmc-bert-pytorch_lightning](https://github.com/jaylnne/nsmc-bert-pytorch_lightning)\n",
    "\n",
    "<br><b>Blog</b>\n",
    "<br>[Week 28 - BERT만 잘 써먹어도 최고가 될 수 있다?](https://jiho-ml.com/weekly-nlp-28/)\n",
    "<br>[Week 37 - NLP 모델, 낚시성 기사 방지 효과 검증돼](https://jiho-ml.com/weekly-nlp-37/)\n",
    "\n",
    "<br><b>Dacon</b>\n",
    "<br>[[private 3rd] klue/roberta + teacher-student](https://dacon.io/en/competitions/official/235747/codeshare/3072)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
